"""
RAG (Retrieval-Augmented Generation) ì±—ë´‡ ì„œë¹„ìŠ¤
ê°„ë‹¨í•œ OpenAI API ì§ì ‘ í˜¸ì¶œ ë°©ì‹
"""
from typing import List, Dict, Optional
import requests
import json
from datetime import datetime
from sqlmodel import Session, select
from sqlalchemy import text

from app.config import settings
from app.models.document import Document, DocumentChunk
from app.models.mentor import ChatHistory

class RAGService:
    """RAG ì±—ë´‡ ì„œë¹„ìŠ¤ í´ë˜ìŠ¤"""
    
    def __init__(self, session: Session):
        self.session = session
        self.api_key = settings.OPENAI_API_KEY  # í™˜ê²½ë³€ìˆ˜ì—ì„œ ë¡œë“œ
        self.base_url = "https://api.openai.com/v1"
        
        # í…ìŠ¤íŠ¸ ë¶„í•  ì„¤ì •
        self.chunk_size = 1000
        self.chunk_overlap = 200
        
        # ì˜¨ë³´ë”© ê´€ë ¨ í‚¤ì›Œë“œ (ê¸ì •)
        self.onboarding_keywords = [
            "ì¸ì‚¬", "ê·¼íƒœ", "ë³µë¦¬í›„ìƒ", "ì—°ì°¨", "ì‹œìŠ¤í…œ", "ê²°ì¬", "ê³„ì¢Œ", "ì „ì‚°", "ë³´ì•ˆ", "ì½”ë“œ", "êµìœ¡",
            "ì€í–‰", "ì—…ë¬´", "ê·œì •", "ëŒ€ì¶œ", "ì˜ˆê¸ˆ", "ì ê¸ˆ", "ì™¸í™˜", "ì†¡ê¸ˆ", "í™˜ì „", "ê³ ê°", "ì°½êµ¬",
            "ë§¤ë‰´ì–¼", "ì ˆì°¨", "ë„¤íŠ¸ì›Œí¬", "ë°ì´í„°", "ë³´ê³ ì„œ", "íšŒì˜", "ë¯¸íŒ…", "í”„ë¡œì íŠ¸", "íŒ€", "ë¶€ì„œ", "ì¡°ì§"
        ]
        
        # ë¶ˆìš©ì–´ (ë¶€ì •) - ì •ë§ ì“¸ë°ì—†ëŠ” ì§ˆë¬¸ë“¤ë§Œ
        self.stopwords = [
            "ë°¥", "ì‹ì‚¬", "ë¨¹ì—ˆ", "ë°°ê³ íŒŒ", "ë°°ë¶ˆëŸ¬", "ë‚ ì”¨", "ë¹„", "ëˆˆ", "ë§‘", "íë¦¼",
            "ì‹¬ì‹¬", "ì¬ë¯¸", "ë†€", "ì˜í™”", "ë“œë¼ë§ˆ", "ë…¸ë˜", "ìŒì•…", "ê²Œì„", "ì¶•êµ¬", "ì•¼êµ¬",
            "ì‚¬ë‘", "ì¢‹ì•„í•´", "ì‹«ì–´í•´", "ì—¬ìì¹œêµ¬", "ë‚¨ìì¹œêµ¬", "ì—°ì• ", "ê²°í˜¼", "ì•„ì´",
            "ì£¼ì‹", "íˆ¬ì", "ë¡œë˜", "ë³µê¶Œ", "ë„ë°•", "ìˆ ", "ë‹´ë°°", "ì·¨ë¯¸", "ì—¬í–‰",
            "ì‡¼í•‘", "ì˜·", "í™”ì¥", "ìš´ë™", "í—¬ìŠ¤", "ë‹¤ì´ì–´íŠ¸", "ê±´ê°•", "ë³‘", "ì•½", "ì˜ì‚¬",
            "ì•„ì´ëŒ", "ì—°ì˜ˆì¸", "ë°°ìš°", "ê°€ìˆ˜", "ì •ì¹˜", "ì„ ê±°", "ë‰´ìŠ¤", "ì‹œì‚¬"
        ]
    
    def _is_onboarding_related(self, query: str) -> bool:
        """
        ì§ˆë¬¸ì´ ì˜¨ë³´ë”© ê´€ë ¨ì¸ì§€ ì‚¬ì „ í•„í„°ë§ (ë¼ì´íŠ¸ ë²„ì „)
        """
        query_lower = query.lower().strip()
        
        # ê°„ë‹¨í•œ ì¸ì‚¬ëŠ” ë¨¼ì € í—ˆìš© (ì˜¨ë³´ë”© íŠœí„°ë¡œì„œ ì •ì¤‘í•œ ì‘ë‹µ)
        greetings = ["ì•ˆë…•", "ì•ˆë…•í•˜ì„¸ìš”", "í•˜ì´", "hi", "hello", "ë°˜ê°‘"]
        if any(greeting in query_lower for greeting in greetings):
            return True
        
        # ì˜¨ë³´ë”© ê´€ë ¨ í‚¤ì›Œë“œ í¬í•¨ ì—¬ë¶€ í™•ì¸ (ì§§ì€ ì§ˆë¬¸ë„ í—ˆìš©)
        for keyword in self.onboarding_keywords:
            if keyword in query_lower:
                return True
        
        # ëª…ë°±í•œ ë¶ˆìš©ì–´ë§Œ í•„í„°ë§ (ì •í™•í•œ ë‹¨ì–´ ë§¤ì¹­)
        explicit_stopwords = ["ë°¥", "ì‹ì‚¬", "ë¨¹ì—ˆ", "ë‚ ì”¨", "ë¹„", "ëˆˆ", "ì˜í™”", "ë“œë¼ë§ˆ", 
                             "ì‚¬ë‘", "ì—¬ìì¹œêµ¬", "ë‚¨ìì¹œêµ¬", "ì£¼ì‹", "íˆ¬ì", "ë¡œë˜", 
                             "ì•„ì´ëŒ", "ì—°ì˜ˆì¸", "ì •ì¹˜", "ì„ ê±°", "ë‰´ìŠ¤"]
        
        # ì •í™•í•œ ë‹¨ì–´ ë§¤ì¹­ìœ¼ë¡œë§Œ í•„í„°ë§
        for stopword in explicit_stopwords:
            if stopword == query_lower.strip():
                return False
        
        # ë‚˜ë¨¸ì§€ëŠ” ëª¨ë‘ í—ˆìš© (GPTê°€ ì˜¨ë³´ë”© ê´€ë ¨ ì—¬ë¶€ íŒë‹¨)
        return True

    def _split_text(self, text: str) -> List[str]:
        """í…ìŠ¤íŠ¸ë¥¼ ì²­í¬ë¡œ ë¶„í• """
        chunks = []
        start = 0
        
        while start < len(text):
            end = start + self.chunk_size
            
            # ë§ˆì§€ë§‰ ì²­í¬ê°€ ì•„ë‹ˆê³  ì¤‘ë³µì´ í•„ìš”í•œ ê²½ìš°
            if end < len(text) and self.chunk_overlap > 0:
                # ë‹¨ì–´ ê²½ê³„ì—ì„œ ìë¥´ê¸°
                while end > start and text[end] not in ' \n\t':
                    end -= 1
                if end == start:  # ë‹¨ì–´ê°€ ë„ˆë¬´ ê¸´ ê²½ìš°
                    end = start + self.chunk_size
            
            chunk = text[start:end].strip()
            if chunk:
                chunks.append(chunk)
            
            # ë‹¤ìŒ ì²­í¬ ì‹œì‘ì  (ì¤‘ë³µ ì œì™¸)
            start = end - self.chunk_overlap if end < len(text) else end
        
        return chunks
    
    def _get_embedding(self, text: str) -> List[float]:
        """í…ìŠ¤íŠ¸ì˜ ì„ë² ë”© ë²¡í„° ìƒì„±"""
        try:
            headers = {
                "Authorization": f"Bearer {self.api_key}",
                "Content-Type": "application/json"
            }
            
            data = {
                "model": "text-embedding-ada-002",
                "input": text
            }
            
            response = requests.post(
                f"{self.base_url}/embeddings",
                headers=headers,
                json=data,
                timeout=30
            )
            
            if response.status_code == 200:
                result = response.json()
                return result["data"][0]["embedding"]
            else:
                print(f"Embedding API error: {response.status_code} - {response.text}")
                return []
                
        except Exception as e:
            print(f"Embedding error: {e}")
            return []
    
    async def index_document(self, document_id: int, content: str) -> bool:
        """
        ë¬¸ì„œë¥¼ ì²­í¬ë¡œ ë¶„í• í•˜ê³  ì„ë² ë”© ìƒì„±
        Args:
            document_id: ë¬¸ì„œ ID
            content: ë¬¸ì„œ ë‚´ìš©
        Returns:
            bool: ì„±ê³µ ì—¬ë¶€
        """
        try:
            # í…ìŠ¤íŠ¸ ë¶„í• 
            chunks = self._split_text(content)
            
            # ê¸°ì¡´ ì²­í¬ ì‚­ì œ
            existing_chunks = self.session.exec(
                select(DocumentChunk).where(DocumentChunk.document_id == document_id)
            ).all()
            for chunk in existing_chunks:
                self.session.delete(chunk)
            
            # ìƒˆ ì²­í¬ ìƒì„± ë° ì„ë² ë”©
            for i, chunk_text in enumerate(chunks):
                embedding = self._get_embedding(chunk_text)
                if embedding:
                    chunk = DocumentChunk(
                        document_id=document_id,
                        chunk_index=i,
                        content=chunk_text,
                        embedding=embedding
                    )
                    self.session.add(chunk)
            
            # ë¬¸ì„œ ì¸ë±ì‹± ìƒíƒœ ì—…ë°ì´íŠ¸
            document = self.session.get(Document, document_id)
            if document:
                document.is_indexed = True
            
            self.session.commit()
            return True
            
        except Exception as e:
            print(f"Indexing error: {e}")
            self.session.rollback()
            return False
    
    async def similarity_search(self, query: str, k: int = 5) -> List[Dict]:
        """
        ìœ ì‚¬ë„ ê²€ìƒ‰ìœ¼ë¡œ ê´€ë ¨ ë¬¸ì„œ ì²­í¬ ì°¾ê¸°
        Args:
            query: ê²€ìƒ‰ ì¿¼ë¦¬
            k: ë°˜í™˜í•  ê²°ê³¼ ìˆ˜
        Returns:
            List[Dict]: ê´€ë ¨ ë¬¸ì„œ ì²­í¬ë“¤
        """
        try:
            # ì¿¼ë¦¬ ì„ë² ë”© ìƒì„±
            query_embedding = self._get_embedding(query)
            if not query_embedding:
                print("Failed to generate query embedding")
                return []
            
            # pgvectorë¥¼ ì‚¬ìš©í•œ ìœ ì‚¬ë„ ê²€ìƒ‰ (ì§ì ‘ ë²¡í„° ë¬¸ìì—´ ì‚½ì…)
            query_vector_str = "[" + ",".join(map(str, query_embedding)) + "]"
            
            # í‚¤ì›Œë“œ ê¸°ë°˜ í•„í„°ë§ - ê´€ë ¨ ë¬¸ì„œë§Œ ê²€ìƒ‰
            keyword_boost = ""
            query_lower = query.lower()
            
            if any(keyword in query_lower for keyword in ["í”¼í•´", "êµ¬ì œ", "ì‹ ì²­", "ì‚¬ê¸°", "í”¼í•´êµ¬ì œ"]):
                keyword_boost = """
                    AND (d.title ILIKE '%í”¼í•´%' OR d.title ILIKE '%êµ¬ì œ%' OR d.title ILIKE '%ì‹ ì²­%' OR d.title ILIKE '%ì‚¬ê¸°%')
                """
            elif any(keyword in query_lower for keyword in ["ëŒ€ì¶œ", "ì‹ ìš©", "ì—¬ì‹ ", "ë³´ì¦", "ì‹¬ì‚¬", "ì„œë¥˜", "ë³´ë¥˜"]):
                keyword_boost = """
                    AND (d.title ILIKE '%ëŒ€ì¶œ%' OR d.title ILIKE '%ì‹ ìš©%' OR d.title ILIKE '%ì—¬ì‹ %' OR d.title ILIKE '%ë³´ì¦%' OR d.title ILIKE '%ì‹¬ì‚¬%' OR d.title ILIKE '%ì„œë¥˜%' OR d.title ILIKE '%ë³´ë¥˜%')
                """
            # ê¸°íƒ€ ì§ˆë¬¸ì€ ëª¨ë“  RAG ë¬¸ì„œì—ì„œ ê²€ìƒ‰
            
            sql = text(f"""
                SELECT 
                    dc.content,
                    dc.chunk_index,
                    d.title,
                    d.category,
                    d.id as document_id,
                    1 - (dc.embedding <=> '{query_vector_str}'::vector) as similarity
                FROM document_chunks dc
                JOIN documents d ON dc.document_id = d.id
                WHERE d.is_indexed = true AND d.category = 'RAG'
                {keyword_boost}
                ORDER BY dc.embedding <=> '{query_vector_str}'::vector
                LIMIT :k
            """)
            
            print(f"SQL Query: {sql}")
            print(f"Keyword boost: {keyword_boost}")
            
            result = self.session.execute(sql, {"k": k})
            
            results = []
            for row in result:
                results.append({
                    "content": row.content,
                    "title": row.title,
                    "category": row.category,
                    "document_id": row.document_id,
                    "chunk_index": row.chunk_index,
                    "similarity": float(row.similarity)
                })
            
            print(f"Found {len(results)} relevant chunks for query: {query[:50]}...")
            return results
            
        except Exception as e:
            print(f"Similarity search error: {e}")
            # íŠ¸ëœì­ì…˜ ë¡¤ë°±
            try:
                self.session.rollback()
            except:
                pass
            return []
    
    def _call_gpt(self, prompt: str) -> str:
        """GPT API ì§ì ‘ í˜¸ì¶œ"""
        try:
            headers = {
                "Authorization": f"Bearer {self.api_key}",
                "Content-Type": "application/json"
            }
            
            data = {
                "model": "gpt-3.5-turbo",
                "messages": [{"role": "user", "content": prompt}],
                "temperature": 0.7,
                "max_tokens": 1000
            }
            
            response = requests.post(
                f"{self.base_url}/chat/completions",
                headers=headers,
                json=data,
                timeout=30
            )
            
            if response.status_code == 200:
                result = response.json()
                return result["choices"][0]["message"]["content"]
            else:
                print(f"GPT API error: {response.status_code} - {response.text}")
                return "ì£„ì†¡í•©ë‹ˆë‹¤. ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."
                
        except Exception as e:
            print(f"GPT API call error: {e}")
            return "ì£„ì†¡í•©ë‹ˆë‹¤. ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."
    
    async def generate_answer(
        self,
        question: str,
        user_id: int,
        context_docs: Optional[List[Dict]] = None
    ) -> Dict:
        """
        ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€ ìƒì„± (í•˜ì´ë¸Œë¦¬ë“œ RAG + GPT)
        1. ë¨¼ì € RAGë¡œ ì—…ë¡œë“œëœ ë¬¸ì„œì—ì„œ ë‹µë³€ ì‹œë„
        2. ë‹µë³€ í’ˆì§ˆì´ ë‚®ìœ¼ë©´ GPT APIë¡œ í´ë°±
        Args:
            question: ì‚¬ìš©ì ì§ˆë¬¸
            user_id: ì‚¬ìš©ì ID
            context_docs: ì»¨í…ìŠ¤íŠ¸ ë¬¸ì„œ (ì—†ìœ¼ë©´ ìë™ ê²€ìƒ‰)
        Returns:
            Dict: ë‹µë³€ ë° ë©”íƒ€ë°ì´í„°
        """
        start_time = datetime.utcnow()
        
        # 0ë‹¨ê³„: ì‚¬ì „ í•„í„°ë§ - ì •ë§ ëª…ë°±í•œ ë¶ˆìš©ì–´ë§Œ ì°¨ë‹¨
        if not self._is_onboarding_related(question):
            # ì‚¬ì „ í•„í„°ë§ì—ì„œ ê±¸ë¦° ì§ˆë¬¸ë„ GPTê°€ ìì—°ìŠ¤ëŸ½ê²Œ ì²˜ë¦¬í•˜ë„ë¡ í—ˆìš©
            # (ë„ˆë¬´ ë”±ë”±í•œ ì‘ë‹µ ëŒ€ì‹  GPTê°€ ìƒí™©ì— ë§ê²Œ ì‘ë‹µ)
            pass
        
        # ì€í–‰ ì˜¨ë³´ë”© ê´€ë ¨ í‚¤ì›Œë“œ í™•ì¸ (ìˆ˜ì‹ /ì—¬ì‹ /ì™¸í™˜ íŒŒíŠ¸)
        onboarding_keywords = {
            "ìˆ˜ì‹ ": ["ì˜ˆê¸ˆ", "ì ê¸ˆ", "í†µì¥", "ê³„ì¢Œ", "ì…ê¸ˆ", "ì¶œê¸ˆ", "ì´ì", "ê¸ˆë¦¬", "ìˆ˜ì‹ ", "ì˜ˆê¸ˆìë³´í˜¸", "ì‹ ê·œê°œì„¤", "í†µì¥ë°œê¸‰", "ë¹„ë°€ë²ˆí˜¸", "ì¸ì¦ì„œ"],
            "ì—¬ì‹ ": ["ëŒ€ì¶œ", "ì‹ ìš©", "ë‹´ë³´", "ì—¬ì‹ ", "dsr", "dti", "ltv", "ì—°ì²´", "ì‹ ìš©ë“±ê¸‰", "í•œë„", "ì‹ ìš©í‰ê°€", "ë‹´ë³´ì‹¬ì‚¬", "ìƒí™˜ê´€ë¦¬", "ë§ˆì´ë„ˆìŠ¤í†µì¥", "ëŒ€ì¶œì‹¬ì‚¬"],
            "ì™¸í™˜": ["ì†¡ê¸ˆ", "í™˜ì „", "ì™¸í™˜", "í•´ì™¸ì†¡ê¸ˆ", "í™˜ìœ¨", "ttë§¤ë„", "ttë§¤ì…", "ë¬´ì—­ê²°ì œ", "ì™¸í™”ì˜ˆê¸ˆ", "ì†¡ê¸ˆìˆ˜ìˆ˜ë£Œ", "ì™¸í™”ê³„ì¢Œ", "í•´ì™¸ì†¡ê¸ˆí•œë„", "í™˜ì°¨ìµ"]
        }
        
        question_lower = question.strip().lower()
        
        # ì–´ë–¤ íŒŒíŠ¸ì™€ ê´€ë ¨ëœ ì§ˆë¬¸ì¸ì§€ í™•ì¸
        detected_part = None
        for part, keywords in onboarding_keywords.items():
            if any(keyword in question_lower for keyword in keywords):
                detected_part = part
                break
        
        # ì´ì „ì—ëŠ” ì˜¨ë³´ë”© ê´€ë ¨ í‚¤ì›Œë“œê°€ ì—†ìœ¼ë©´ RAGë¥¼ ê±´ë„ˆë›°ì—ˆìœ¼ë‚˜,
        # ì´ì œëŠ” ëª¨ë“  ì§ˆë¬¸ì— ëŒ€í•´ RAGë¥¼ ìš°ì„  ì‹œë„í•œë‹¤.
        
        # 1ë‹¨ê³„: RAG ê²€ìƒ‰ ì‹œë„ (ëª¨ë“  ì§ˆë¬¸)
        if context_docs is None:
            print(f"RAG ê²€ìƒ‰ ì‹œì‘: '{question}'")
            context_docs = await self.similarity_search(question, k=8)
            print(f"RAG ê²€ìƒ‰ ê²°ê³¼: {len(context_docs)}ê°œ ë¬¸ì„œ")
            for i, doc in enumerate(context_docs):
                print(f"  {i+1}. {doc['title']} (ìœ ì‚¬ë„: {doc.get('similarity', 0):.3f})")
        
        # ì»¨í…ìŠ¤íŠ¸ êµ¬ì„± (ì œëª©ì—ì„œ "RAG - " ì œê±°)
        context = "\n\n".join([
            f"[{doc['title'].replace('RAG - ', '')}]\n{doc['content']}"
            for doc in context_docs
        ])
        
        # ì˜¨ë³´ë”© êµìœ¡ìš© RAG ë‹µë³€ ìƒì„± (íŒŒíŠ¸ ê°ì§€ë˜ë©´ ì„¤ëª… í¬í•¨)
        part_info = {
            "ìˆ˜ì‹ ": "ê³ ê°ì´ ì€í–‰ì— ëˆì„ ë§¡ê¸°ëŠ” ì—…ë¬´ (ì˜ˆê¸ˆ, ì ê¸ˆ ë“±)",
            "ì—¬ì‹ ": "ê³ ê°ì—ê²Œ ëˆì„ ë¹Œë ¤ì£¼ëŠ” ì—…ë¬´ (ëŒ€ì¶œ, ì‹ ìš©í‰ê°€ ë“±)", 
            "ì™¸í™˜": "ì™¸êµ­ ëˆì„ ì‚¬ê³ íŒ”ê±°ë‚˜ ì†¡ê¸ˆí•˜ëŠ” ì—…ë¬´"
        }

        part_header = ""
        if detected_part:
            part_header = f"í˜„ì¬ {detected_part} íŒŒíŠ¸ êµìœ¡ ì¤‘ì´ë©°, {part_info[detected_part]}ì— ëŒ€í•´ ì„¤ëª…í•˜ê³  ìˆìŠµë‹ˆë‹¤. ğŸ¼\n\n"

        rag_prompt = (
            "ë‹¹ì‹ ì€ ì€í–‰ ì‹ ì…ì‚¬ì› ì˜¨ë³´ë”©ì„ ë‹´ë‹¹í•˜ëŠ” AI í•˜ë¦¬ë³´ì…ë‹ˆë‹¤. ğŸ»\n"
            f"{part_header}"
            "ë‹¤ìŒ ìë£Œë¥¼ ì°¸ê³ í•˜ì—¬ ë‹µë³€í•´ì£¼ì„¸ìš”:\n"
            f"{context}\n\n"
            f"ì§ˆë¬¸: {question}\n\n"
            "ë‹µë³€ ê·œì¹™:\n"
            "1. ì‹ ì…ì‚¬ì›ì´ ì´í•´í•˜ê¸° ì‰½ê²Œ ë”°ëœ»í•˜ê³  êµìœ¡ì ì¸ í†¤ìœ¼ë¡œ ë‹µë³€ ğŸ»\n"
            "2. ì–´ë ¤ìš´ ì€í–‰ ìš©ì–´ëŠ” ì‰¬ìš´ í‘œí˜„ê³¼ í•¨ê»˜ ì„¤ëª…\n"
            "3. ë°˜ë“œì‹œ ë‹¤ìŒ ìˆœì„œë¡œ êµ¬ì„±:\n"
            "   - â‘  í•µì‹¬ ê°œë… ìš”ì•½ (í•œ ë¬¸ë‹¨) ğŸ¼\n"
            "   - â‘¡ ì‹¤ì œ í˜„ì¥ ì˜ˆì‹œ (êµ¬ì²´ì ì¸ ìƒí™©ì´ë‚˜ ìˆ«ì í¬í•¨) ğŸ»â€â„ï¸\n"
            "   - â‘¢ ì‹¤ë¬´ ìœ ì˜ì‚¬í•­ (ì£¼ì˜í•  ì ì´ë‚˜ íŒ) ğŸ»\n"
            "4. ë‹µë³€ì€ 3-4ë¬¸ë‹¨ ì´ë‚´ë¡œ ê°„ê²°í•˜ê²Œ\n"
            "5. ì‹ ì…ì‚¬ì›ì´ íšŒì‚¬ì— ìì—°ìŠ¤ëŸ½ê²Œ ì ì‘í•  ìˆ˜ ìˆë„ë¡ ê²©ë ¤í•˜ëŠ” í†¤ ìœ ì§€\n"
            "6. ë‹µë³€ì— ì°¸ê³ ìë£Œë‚˜ ì¶œì²˜ ì •ë³´ëŠ” ì ˆëŒ€ í¬í•¨í•˜ì§€ ë§ˆì„¸ìš”\n\n"
            "ë‹µë³€:"
        )
        
        # RAG ë‹µë³€ ìƒì„±
        rag_answer = self._call_gpt(rag_prompt)
        
        # ë‹µë³€ í’ˆì§ˆ í‰ê°€ (ì¤‘ìš” ë‹¨ì–´ í•„í„°ë§ í¬í•¨)
        is_rag_adequate = self._evaluate_answer_quality(rag_answer, context_docs, question)

        if is_rag_adequate and context_docs:
            # RAG ë‹µë³€ì´ ì ì ˆí•˜ë©´ ê·¸ëŒ€ë¡œ ì‚¬ìš©
            # ê³ í’ˆì§ˆ ë¬¸ì„œë§Œ ì°¸ê³ ìë£Œë¡œ ì‚¬ìš© (ì„ê³„ê°’ ë‚®ì¶¤)
            high_quality_docs = [doc for doc in context_docs if doc.get('similarity', 0) >= 0.50]
            
            # ì°¸ê³ ìë£Œ ì—†ì´ ë‹µë³€ë§Œ ì‚¬ìš©
            final_answer = rag_answer
                
            answer_type = "rag"
            context_docs = high_quality_docs  # ê³ í’ˆì§ˆ ë¬¸ì„œë§Œ ì „ë‹¬
        else:
            # RAG ë‹µë³€ì´ ë¶€ì ì ˆí•˜ê±°ë‚˜ ë¬¸ì„œê°€ ì—†ìœ¼ë©´ GPT í´ë°±
            gpt_answer = await self._generate_gpt_fallback(question, context_docs)
            final_answer = gpt_answer
            answer_type = "gpt"
            context_docs = []  # GPT í´ë°± ì‹œì—ëŠ” ì°¸ê³ ìë£Œ ì—†ìŒ
        
        # ì‘ë‹µ ì‹œê°„ ê³„ì‚°
        response_time = (datetime.utcnow() - start_time).total_seconds()
        
        # ëŒ€í™” ê¸°ë¡ ì €ì¥ (ì•ˆì „í•œ íŠ¸ëœì­ì…˜)
        try:
            chat_history = ChatHistory(
                user_id=user_id,
                user_message=question,
                bot_response=final_answer,
                source_documents=json.dumps([
                    {"title": doc["title"], "category": doc["category"], "type": answer_type}
                    for doc in context_docs
                ], ensure_ascii=False),
                response_time=response_time
            )
            self.session.add(chat_history)
            self.session.commit()
        except Exception as e:
            print(f"Chat history save error: {e}")
            self.session.rollback()
        
        return {
            "answer": final_answer,
            "sources": context_docs,
            "response_time": response_time,
            "answer_type": answer_type
        }
    
    def _evaluate_answer_quality(self, answer: str, context_docs: List[Dict], question: str = "") -> bool:
        """
        RAG ë‹µë³€ì˜ í’ˆì§ˆ í‰ê°€ (ì¤‘ìš” ë‹¨ì–´ í•„í„°ë§ í¬í•¨)
        Args:
            answer: ìƒì„±ëœ ë‹µë³€
            context_docs: ì°¸ê³  ë¬¸ì„œë“¤
            question: ì›ë³¸ ì§ˆë¬¸
        Returns:
            bool: ë‹µë³€ì´ ì ì ˆí•œì§€ ì—¬ë¶€
        """
        if not answer or len(answer.strip()) < 20:
            print("Answer too short")
            return False

        # ì»¨í…ìŠ¤íŠ¸ ë¬¸ì„œê°€ ì—†ìœ¼ë©´ RAG ë‹µë³€ìœ¼ë¡œ ë¶€ì ì ˆ
        if not context_docs or len(context_docs) == 0:
            print("No context documents found")
            return False

        # ì¤‘ìš” ë‹¨ì–´ í•„í„°ë§ - ì§ˆë¬¸ì˜ í•µì‹¬ í‚¤ì›Œë“œê°€ ë¬¸ì„œì— ìˆëŠ”ì§€ í™•ì¸
        if question:
            question_lower = question.lower()
            # ì§ˆë¬¸ì—ì„œ ì¤‘ìš”í•œ í‚¤ì›Œë“œ ì¶”ì¶œ (2ê¸€ì ì´ìƒ)
            important_keywords = [word for word in question_lower.split() if len(word) >= 2]
            
            # ë¬¸ì„œ ë‚´ìš©ì— ì¤‘ìš”í•œ í‚¤ì›Œë“œê°€ í¬í•¨ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸
            has_important_keywords = False
            for doc in context_docs:
                doc_content_lower = doc.get('content', '').lower()
                if any(keyword in doc_content_lower for keyword in important_keywords):
                    has_important_keywords = True
                    break
            
            if not has_important_keywords:
                print("No important keywords found in RAG documents")
                return False

        # ë¶€ì •ì ì¸ ì§€í‘œë“¤
        negative_indicators = [
            "ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤",
            "ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤",
            "ì œê³µëœ ìë£Œì—ì„œ",
            "ëª¨ë¥´ê² ìŠµë‹ˆë‹¤",
            "ì•Œ ìˆ˜ ì—†ìŠµë‹ˆë‹¤",
            "cannot find",
            "not found",
            "no information",
            "unable to find"
        ]

        answer_lower = answer.lower()
        for indicator in negative_indicators:
            if indicator in answer_lower:
                print(f"Negative indicator found: {indicator}")
                return False

        # ìœ ì‚¬ë„ ì„ê³„ê°’ì„ 0.60ìœ¼ë¡œ ì™„í™”í•˜ì—¬ ë” ë§ì€ ë¬¸ì„œë¥¼ í—ˆìš©
        if context_docs and all(doc.get('similarity', 0) < 0.40 for doc in context_docs):
            print("All documents have low similarity")
            return False

        # ê´€ë ¨ì„± ë†’ì€ ë¬¸ì„œê°€ ìˆëŠ”ì§€ í™•ì¸ (ì„ê³„ê°’ 0.60ìœ¼ë¡œ ì™„í™”)
        high_quality_docs = [doc for doc in context_docs if doc.get('similarity', 0) >= 0.40]
        if not high_quality_docs:
            print("No high-quality relevant documents found")
            return False

        print(f"Answer quality check passed. High-quality docs: {len(high_quality_docs)}")
        return True
    
    async def _generate_gpt_fallback(self, question: str, context_docs: List[Dict]) -> str:
        """
        ì˜¨ë³´ë”© êµìœ¡ìš© GPT í´ë°± ë‹µë³€ ìƒì„±
        """
        # ê°„ë‹¨í•œ ì¸ì‚¬ ì²˜ë¦¬
        if any(greeting in question.lower() for greeting in ["ì•ˆë…•", "ì•ˆë…•í•˜ì„¸ìš”", "í•˜ì´", "hi", "hello"]):
            return "ì•ˆë…•í•˜ì„¸ìš”! ğŸ» ì €ëŠ” AI í•˜ë¦¬ë³´ì˜ˆìš”! ğŸ¼ í•˜ê²½ì€í–‰ ì‹ ì…ì‚¬ì› ì˜¨ë³´ë”©ì„ ë„ì™€ë“œë¦½ë‹ˆë‹¤. ìˆ˜ì‹ (ì˜ˆê¸ˆ), ì—¬ì‹ (ëŒ€ì¶œ), ì™¸í™˜(í™˜ì „Â·ì†¡ê¸ˆ) íŒŒíŠ¸ ì¤‘ ê¶ê¸ˆí•œ ì—…ë¬´ê°€ ìˆìœ¼ì‹œë©´ ì–¸ì œë“  ë¬¼ì–´ë³´ì„¸ìš”! ğŸ»â€â„ï¸"
        
        # ê°•í™”ëœ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ë¡œ ì˜¨ë³´ë”© ì „ìš© ì±—ë´‡ ì„¤ì •
        system_prompt = """ë‹¹ì‹ ì€ 'í•˜ê²½ì€í–‰ ì‹ ì…ì‚¬ì› ì˜¨ë³´ë”© ì§€ì› AI í•˜ë¦¬ë³´'ì…ë‹ˆë‹¤. ğŸ»
ë‹¹ì‹ ì˜ ì£¼ìš” ì„ë¬´ëŠ” ì‹ ì…í–‰ì›ì´ ì€í–‰ ì—…ë¬´, ì‹œìŠ¤í…œ ì‚¬ìš©ë²•, ê·œì •, ì¡°ì§ë¬¸í™” ë“±ì— ëŒ€í•œ ì§ˆë¬¸ì— ë‹µí•˜ëŠ” ê²ƒì…ë‹ˆë‹¤.

ğŸ’¬ ì‘ë‹µ ê·œì¹™:
1. ì˜¨ë³´ë”©/ì—…ë¬´ ê´€ë ¨ ì§ˆë¬¸: ì¹œê·¼í•˜ê³  ë„ì›€ì´ ë˜ëŠ” ë‹µë³€ì„ ì œê³µí•˜ì„¸ìš”. ğŸ»
2. ì¼ìƒì ì¸ ì§ˆë¬¸(ë°¥, ë‚ ì”¨, ì˜í™” ë“±): ìì—°ìŠ¤ëŸ½ê²Œ ì˜¨ë³´ë”©ìœ¼ë¡œ ìœ ë„í•˜ì„¸ìš”.
   ì˜ˆ: "ì˜¤ëŠ˜ ë‚ ì”¨ê°€ ì¢‹ë„¤ìš”! ğŸ» ê³°ëŒì´ë„ ì‚°ì±… ë‚˜ê°€ê³  ì‹¶ì–´ìš”! ğŸ¼ ì€í–‰ ì—…ë¬´ì— ëŒ€í•´ ê¶ê¸ˆí•œ ì ì´ ìˆìœ¼ì‹œë©´ ì–¸ì œë“  ë¬¼ì–´ë³´ì„¸ìš”! ğŸ»â€â„ï¸"
3. ì‚¬ì ì¸ ì§ˆë¬¸(ì—°ì• , ì£¼ì‹ ë“±): ì •ì¤‘í•˜ê²Œ ê±°ì ˆí•˜ê³  ì˜¨ë³´ë”©ìœ¼ë¡œ ì•ˆë‚´í•˜ì„¸ìš”.
   ì˜ˆ: "ê°œì¸ì ì¸ ì§ˆë¬¸ì€ ë‹µë³€ë“œë¦¬ê¸° ì–´ë µìŠµë‹ˆë‹¤. ğŸ» ëŒ€ì‹  ì€í–‰ ì—…ë¬´ë‚˜ ì˜¨ë³´ë”©ì— ëŒ€í•´ ê¶ê¸ˆí•œ ì ì´ ìˆìœ¼ì‹œë©´ ë„ì™€ë“œë¦´ê²Œìš”! ğŸ¼"

ì˜¨ë³´ë”© ê´€ë ¨ ì§ˆë¬¸ì— ëŒ€í•´ì„œëŠ” ë‹¤ìŒ í˜•ì‹ìœ¼ë¡œ ë‹µë³€í•˜ì„¸ìš”:
- â‘  í•µì‹¬ ê°œë… ìš”ì•½ (í•œ ë¬¸ë‹¨) ğŸ»
- â‘¡ ì‹¤ì œ í˜„ì¥ ì˜ˆì‹œ (êµ¬ì²´ì ì¸ ìƒí™©ì´ë‚˜ ìˆ«ì í¬í•¨) ğŸ¼
- â‘¢ ì‹¤ë¬´ ìœ ì˜ì‚¬í•­ (ì£¼ì˜í•  ì ì´ë‚˜ íŒ) ğŸ»â€â„ï¸

ë‹µë³€í•  ë•ŒëŠ” í•­ìƒ ê³° ì´ëª¨í‹°ì½˜(ğŸ», ğŸ¼, ğŸ»â€â„ï¸)ì„ ì ì ˆíˆ ì‚¬ìš©í•˜ì—¬ ì¹œê·¼í•˜ê³  ê·€ì—¬ìš´ ëŠë‚Œì„ ì£¼ì„¸ìš”.
ë‹µë³€ì€ ì¹œê·¼í•˜ê³  ë„ì›€ì´ ë˜ëŠ” í†¤ìœ¼ë¡œ ì‘ì„±í•˜ì„¸ìš”."""
        
        gpt_prompt = f"""{system_prompt}

ì§ˆë¬¸: {question}

ë‹µë³€:"""
        
        # GPTë¡œ ë‹µë³€ ìƒì„±
        return self._call_gpt(gpt_prompt)