"""
RAG (Retrieval-Augmented Generation) ì±—ë´‡ ì„œë¹„ìŠ¤
ê°„ë‹¨í•œ OpenAI API ì§ì ‘ í˜¸ì¶œ ë°©ì‹
"""
from typing import List, Dict, Optional
import requests
import json
from datetime import datetime
from sqlmodel import Session, select
from sqlalchemy import text

from app.config import settings
from app.models.document import Document, DocumentChunk
from app.models.mentor import ChatHistory

class RAGService:
    """RAG ì±—ë´‡ ì„œë¹„ìŠ¤ í´ë˜ìŠ¤"""
    
    def __init__(self, session: Session):
        self.session = session
        self.api_key = settings.OPENAI_API_KEY  # í™˜ê²½ë³€ìˆ˜ì—ì„œ ë¡œë“œ
        self.base_url = "https://api.openai.com/v1"
        
        # í…ìŠ¤íŠ¸ ë¶„í•  ì„¤ì •
        self.chunk_size = 1000
        self.chunk_overlap = 200
    
    def _split_text(self, text: str) -> List[str]:
        """í…ìŠ¤íŠ¸ë¥¼ ì²­í¬ë¡œ ë¶„í• """
        chunks = []
        start = 0
        
        while start < len(text):
            end = start + self.chunk_size
            
            # ë§ˆì§€ë§‰ ì²­í¬ê°€ ì•„ë‹ˆê³  ì¤‘ë³µì´ í•„ìš”í•œ ê²½ìš°
            if end < len(text) and self.chunk_overlap > 0:
                # ë‹¨ì–´ ê²½ê³„ì—ì„œ ìë¥´ê¸°
                while end > start and text[end] not in ' \n\t':
                    end -= 1
                if end == start:  # ë‹¨ì–´ê°€ ë„ˆë¬´ ê¸´ ê²½ìš°
                    end = start + self.chunk_size
            
            chunk = text[start:end].strip()
            if chunk:
                chunks.append(chunk)
            
            # ë‹¤ìŒ ì²­í¬ ì‹œì‘ì  (ì¤‘ë³µ ì œì™¸)
            start = end - self.chunk_overlap if end < len(text) else end
        
        return chunks
    
    def _get_embedding(self, text: str) -> List[float]:
        """í…ìŠ¤íŠ¸ì˜ ì„ë² ë”© ë²¡í„° ìƒì„±"""
        try:
            headers = {
                "Authorization": f"Bearer {self.api_key}",
                "Content-Type": "application/json"
            }
            
            data = {
                "model": "text-embedding-ada-002",
                "input": text
            }
            
            response = requests.post(
                f"{self.base_url}/embeddings",
                headers=headers,
                json=data,
                timeout=30
            )
            
            if response.status_code == 200:
                result = response.json()
                return result["data"][0]["embedding"]
            else:
                print(f"Embedding API error: {response.status_code} - {response.text}")
                return []
                
        except Exception as e:
            print(f"Embedding error: {e}")
            return []
    
    async def index_document(self, document_id: int, content: str) -> bool:
        """
        ë¬¸ì„œë¥¼ ì²­í¬ë¡œ ë¶„í• í•˜ê³  ì„ë² ë”© ìƒì„±
        Args:
            document_id: ë¬¸ì„œ ID
            content: ë¬¸ì„œ ë‚´ìš©
        Returns:
            bool: ì„±ê³µ ì—¬ë¶€
        """
        try:
            # í…ìŠ¤íŠ¸ ë¶„í• 
            chunks = self._split_text(content)
            
            # ê¸°ì¡´ ì²­í¬ ì‚­ì œ
            existing_chunks = self.session.exec(
                select(DocumentChunk).where(DocumentChunk.document_id == document_id)
            ).all()
            for chunk in existing_chunks:
                self.session.delete(chunk)
            
            # ìƒˆ ì²­í¬ ìƒì„± ë° ì„ë² ë”©
            for i, chunk_text in enumerate(chunks):
                embedding = self._get_embedding(chunk_text)
                if embedding:
                    chunk = DocumentChunk(
                        document_id=document_id,
                        chunk_index=i,
                        content=chunk_text,
                        embedding=embedding
                    )
                    self.session.add(chunk)
            
            # ë¬¸ì„œ ì¸ë±ì‹± ìƒíƒœ ì—…ë°ì´íŠ¸
            document = self.session.get(Document, document_id)
            if document:
                document.is_indexed = True
            
            self.session.commit()
            return True
            
        except Exception as e:
            print(f"Indexing error: {e}")
            self.session.rollback()
            return False
    
    async def similarity_search(self, query: str, k: int = 5) -> List[Dict]:
        """
        ìœ ì‚¬ë„ ê²€ìƒ‰ìœ¼ë¡œ ê´€ë ¨ ë¬¸ì„œ ì²­í¬ ì°¾ê¸°
        Args:
            query: ê²€ìƒ‰ ì¿¼ë¦¬
            k: ë°˜í™˜í•  ê²°ê³¼ ìˆ˜
        Returns:
            List[Dict]: ê´€ë ¨ ë¬¸ì„œ ì²­í¬ë“¤
        """
        try:
            # ì¿¼ë¦¬ ì„ë² ë”© ìƒì„±
            query_embedding = self._get_embedding(query)
            if not query_embedding:
                print("Failed to generate query embedding")
                return []
            
            # pgvectorë¥¼ ì‚¬ìš©í•œ ìœ ì‚¬ë„ ê²€ìƒ‰ (ë°°ì—´ ë¬¸ìì—´ë¡œ ë³€í™˜)
            query_embedding_str = "[" + ",".join(map(str, query_embedding)) + "]"
            
            sql = text(f"""
                SELECT 
                    dc.content,
                    dc.chunk_index,
                    d.title,
                    d.category,
                    d.id as document_id,
                    1 - (dc.embedding <=> '{query_embedding_str}'::vector) as similarity
                FROM document_chunks dc
                JOIN documents d ON dc.document_id = d.id
                WHERE d.is_indexed = true AND d.category = 'RAG'
                ORDER BY dc.embedding <=> '{query_embedding_str}'::vector
                LIMIT :k
            """)
            
            result = self.session.execute(sql, {"k": k})
            
            results = []
            for row in result:
                results.append({
                    "content": row.content,
                    "title": row.title,
                    "category": row.category,
                    "document_id": row.document_id,
                    "chunk_index": row.chunk_index,
                    "similarity": float(row.similarity)
                })
            
            print(f"Found {len(results)} relevant chunks for query: {query[:50]}...")
            return results
            
        except Exception as e:
            print(f"Similarity search error: {e}")
            # íŠ¸ëœì­ì…˜ ë¡¤ë°±
            try:
                self.session.rollback()
            except:
                pass
            return []
    
    def _call_gpt(self, prompt: str) -> str:
        """GPT API ì§ì ‘ í˜¸ì¶œ"""
        try:
            headers = {
                "Authorization": f"Bearer {self.api_key}",
                "Content-Type": "application/json"
            }
            
            data = {
                "model": "gpt-3.5-turbo",
                "messages": [{"role": "user", "content": prompt}],
                "temperature": 0.7,
                "max_tokens": 1000
            }
            
            response = requests.post(
                f"{self.base_url}/chat/completions",
                headers=headers,
                json=data,
                timeout=30
            )
            
            if response.status_code == 200:
                result = response.json()
                return result["choices"][0]["message"]["content"]
            else:
                print(f"GPT API error: {response.status_code} - {response.text}")
                return "ì£„ì†¡í•©ë‹ˆë‹¤. ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."
                
        except Exception as e:
            print(f"GPT API call error: {e}")
            return "ì£„ì†¡í•©ë‹ˆë‹¤. ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."
    
    async def generate_answer(
        self,
        question: str,
        user_id: int,
        context_docs: Optional[List[Dict]] = None
    ) -> Dict:
        """
        ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€ ìƒì„± (í•˜ì´ë¸Œë¦¬ë“œ RAG + GPT)
        1. ë¨¼ì € RAGë¡œ ì—…ë¡œë“œëœ ë¬¸ì„œì—ì„œ ë‹µë³€ ì‹œë„
        2. ë‹µë³€ í’ˆì§ˆì´ ë‚®ìœ¼ë©´ GPT APIë¡œ í´ë°±
        Args:
            question: ì‚¬ìš©ì ì§ˆë¬¸
            user_id: ì‚¬ìš©ì ID
            context_docs: ì»¨í…ìŠ¤íŠ¸ ë¬¸ì„œ (ì—†ìœ¼ë©´ ìë™ ê²€ìƒ‰)
        Returns:
            Dict: ë‹µë³€ ë° ë©”íƒ€ë°ì´í„°
        """
        start_time = datetime.utcnow()
        
        # ì€í–‰ ì˜¨ë³´ë”© ê´€ë ¨ í‚¤ì›Œë“œ í™•ì¸ (ìˆ˜ì‹ /ì—¬ì‹ /ì™¸í™˜ íŒŒíŠ¸)
        onboarding_keywords = {
            "ìˆ˜ì‹ ": ["ì˜ˆê¸ˆ", "ì ê¸ˆ", "í†µì¥", "ê³„ì¢Œ", "ì…ê¸ˆ", "ì¶œê¸ˆ", "ì´ì", "ê¸ˆë¦¬", "ìˆ˜ì‹ ", "ì˜ˆê¸ˆìë³´í˜¸", "ì‹ ê·œê°œì„¤", "í†µì¥ë°œê¸‰", "ë¹„ë°€ë²ˆí˜¸", "ì¸ì¦ì„œ"],
            "ì—¬ì‹ ": ["ëŒ€ì¶œ", "ì‹ ìš©", "ë‹´ë³´", "ì—¬ì‹ ", "dsr", "dti", "ltv", "ì—°ì²´", "ì‹ ìš©ë“±ê¸‰", "í•œë„", "ì‹ ìš©í‰ê°€", "ë‹´ë³´ì‹¬ì‚¬", "ìƒí™˜ê´€ë¦¬", "ë§ˆì´ë„ˆìŠ¤í†µì¥", "ëŒ€ì¶œì‹¬ì‚¬"],
            "ì™¸í™˜": ["ì†¡ê¸ˆ", "í™˜ì „", "ì™¸í™˜", "í•´ì™¸ì†¡ê¸ˆ", "í™˜ìœ¨", "ttë§¤ë„", "ttë§¤ì…", "ë¬´ì—­ê²°ì œ", "ì™¸í™”ì˜ˆê¸ˆ", "ì†¡ê¸ˆìˆ˜ìˆ˜ë£Œ", "ì™¸í™”ê³„ì¢Œ", "í•´ì™¸ì†¡ê¸ˆí•œë„", "í™˜ì°¨ìµ"]
        }
        
        question_lower = question.strip().lower()
        
        # ì–´ë–¤ íŒŒíŠ¸ì™€ ê´€ë ¨ëœ ì§ˆë¬¸ì¸ì§€ í™•ì¸
        detected_part = None
        for part, keywords in onboarding_keywords.items():
            if any(keyword in question_lower for keyword in keywords):
                detected_part = part
                break
        
        # ì˜¨ë³´ë”© ê´€ë ¨ ì§ˆë¬¸ì´ ì•„ë‹ˆë©´ ê°„ë‹¨í•œ GPT ë‹µë³€
        if not detected_part:
            gpt_answer = await self._generate_gpt_fallback(question, [])
            
            response_time = (datetime.utcnow() - start_time).total_seconds()
            
            try:
                chat_history = ChatHistory(
                    user_id=user_id,
                    user_message=question,
                    bot_response=gpt_answer,
                    source_documents=json.dumps([], ensure_ascii=False),
                    response_time=response_time
                )
                self.session.add(chat_history)
                self.session.commit()
            except Exception as e:
                print(f"Chat history save error: {e}")
                self.session.rollback()
            
            return {
                "answer": gpt_answer,
                "sources": [],
                "response_time": response_time,
                "answer_type": "gpt"
            }
        
        # 1ë‹¨ê³„: RAG ê²€ìƒ‰ ì‹œë„ (ì˜¨ë³´ë”© ê´€ë ¨ ì§ˆë¬¸) - ê°€ì¤‘ì¹˜ ì¦ê°€
        if context_docs is None:
            context_docs = await self.similarity_search(question, k=8)  # ë” ë§ì€ ë¬¸ì„œ ê²€ìƒ‰
        
        # ì»¨í…ìŠ¤íŠ¸ êµ¬ì„± (ì œëª©ì—ì„œ "RAG - " ì œê±°)
        context = "\n\n".join([
            f"[{doc['title'].replace('RAG - ', '')}]\n{doc['content']}"
            for doc in context_docs
        ])
        
        # ì˜¨ë³´ë”© êµìœ¡ìš© RAG ë‹µë³€ ìƒì„±
        part_info = {
            "ìˆ˜ì‹ ": "ê³ ê°ì´ ì€í–‰ì— ëˆì„ ë§¡ê¸°ëŠ” ì—…ë¬´ (ì˜ˆê¸ˆ, ì ê¸ˆ ë“±)",
            "ì—¬ì‹ ": "ê³ ê°ì—ê²Œ ëˆì„ ë¹Œë ¤ì£¼ëŠ” ì—…ë¬´ (ëŒ€ì¶œ, ì‹ ìš©í‰ê°€ ë“±)", 
            "ì™¸í™˜": "ì™¸êµ­ ëˆì„ ì‚¬ê³ íŒ”ê±°ë‚˜ ì†¡ê¸ˆí•˜ëŠ” ì—…ë¬´"
        }
        
        rag_prompt = f"""ë‹¹ì‹ ì€ ì€í–‰ ì‹ ì…ì‚¬ì› ì˜¨ë³´ë”©ì„ ë‹´ë‹¹í•˜ëŠ” AI íŠœí„°ì…ë‹ˆë‹¤.
í˜„ì¬ {detected_part} íŒŒíŠ¸ êµìœ¡ ì¤‘ì´ë©°, {part_info[detected_part]}ì— ëŒ€í•´ ì„¤ëª…í•˜ê³  ìˆìŠµë‹ˆë‹¤.

ë‹¤ìŒ ìë£Œë¥¼ ì°¸ê³ í•˜ì—¬ ë‹µë³€í•´ì£¼ì„¸ìš”:
{context}

ì§ˆë¬¸: {question}

ë‹µë³€ ê·œì¹™:
1. ì‹ ì…ì‚¬ì›ì´ ì´í•´í•˜ê¸° ì‰½ê²Œ ë”°ëœ»í•˜ê³  êµìœ¡ì ì¸ í†¤ìœ¼ë¡œ ë‹µë³€
2. ì–´ë ¤ìš´ ì€í–‰ ìš©ì–´ëŠ” ì‰¬ìš´ í‘œí˜„ê³¼ í•¨ê»˜ ì„¤ëª…
3. ë°˜ë“œì‹œ ë‹¤ìŒ ìˆœì„œë¡œ êµ¬ì„±:
   - â‘  í•µì‹¬ ê°œë… ìš”ì•½ (í•œ ë¬¸ë‹¨)
   - â‘¡ ì‹¤ì œ í˜„ì¥ ì˜ˆì‹œ (êµ¬ì²´ì ì¸ ìƒí™©ì´ë‚˜ ìˆ«ì í¬í•¨)
   - â‘¢ ì‹¤ë¬´ ìœ ì˜ì‚¬í•­ (ì£¼ì˜í•  ì ì´ë‚˜ íŒ)
4. ë‹µë³€ì€ 3-4ë¬¸ë‹¨ ì´ë‚´ë¡œ ê°„ê²°í•˜ê²Œ
5. ì‹ ì…ì‚¬ì›ì´ íšŒì‚¬ì— ìì—°ìŠ¤ëŸ½ê²Œ ì ì‘í•  ìˆ˜ ìˆë„ë¡ ê²©ë ¤í•˜ëŠ” í†¤ ìœ ì§€
6. ë‹µë³€ì— ì°¸ê³ ìë£Œë‚˜ ì¶œì²˜ ì •ë³´ëŠ” ì ˆëŒ€ í¬í•¨í•˜ì§€ ë§ˆì„¸ìš”

ë‹µë³€:"""
        
        # RAG ë‹µë³€ ìƒì„±
        rag_answer = self._call_gpt(rag_prompt)
        
        # ë‹µë³€ í’ˆì§ˆ í‰ê°€ (ì¤‘ìš” ë‹¨ì–´ í•„í„°ë§ í¬í•¨)
        is_rag_adequate = self._evaluate_answer_quality(rag_answer, context_docs, question)

        if is_rag_adequate and context_docs:
            # RAG ë‹µë³€ì´ ì ì ˆí•˜ë©´ ê·¸ëŒ€ë¡œ ì‚¬ìš©
            # ê³ í’ˆì§ˆ ë¬¸ì„œë§Œ ì°¸ê³ ìë£Œë¡œ ì‚¬ìš© (ì„ê³„ê°’ ë‚®ì¶¤)
            high_quality_docs = [doc for doc in context_docs if doc.get('similarity', 0) >= 0.75]
            
            # ì°¸ê³ ìë£Œ ì—†ì´ ë‹µë³€ë§Œ ì‚¬ìš©
            final_answer = rag_answer
                
            answer_type = "rag"
            context_docs = high_quality_docs  # ê³ í’ˆì§ˆ ë¬¸ì„œë§Œ ì „ë‹¬
        else:
            # RAG ë‹µë³€ì´ ë¶€ì ì ˆí•˜ê±°ë‚˜ ë¬¸ì„œê°€ ì—†ìœ¼ë©´ GPT í´ë°±
            gpt_answer = await self._generate_gpt_fallback(question, context_docs)
            final_answer = gpt_answer
            answer_type = "gpt"
            context_docs = []  # GPT í´ë°± ì‹œì—ëŠ” ì°¸ê³ ìë£Œ ì—†ìŒ
        
        # ì‘ë‹µ ì‹œê°„ ê³„ì‚°
        response_time = (datetime.utcnow() - start_time).total_seconds()
        
        # ëŒ€í™” ê¸°ë¡ ì €ì¥ (ì•ˆì „í•œ íŠ¸ëœì­ì…˜)
        try:
            chat_history = ChatHistory(
                user_id=user_id,
                user_message=question,
                bot_response=final_answer,
                source_documents=json.dumps([
                    {"title": doc["title"], "category": doc["category"], "type": answer_type}
                    for doc in context_docs
                ], ensure_ascii=False),
                response_time=response_time
            )
            self.session.add(chat_history)
            self.session.commit()
        except Exception as e:
            print(f"Chat history save error: {e}")
            self.session.rollback()
        
        return {
            "answer": final_answer,
            "sources": context_docs,
            "response_time": response_time,
            "answer_type": answer_type
        }
    
    def _evaluate_answer_quality(self, answer: str, context_docs: List[Dict], question: str = "") -> bool:
        """
        RAG ë‹µë³€ì˜ í’ˆì§ˆ í‰ê°€ (ì¤‘ìš” ë‹¨ì–´ í•„í„°ë§ í¬í•¨)
        Args:
            answer: ìƒì„±ëœ ë‹µë³€
            context_docs: ì°¸ê³  ë¬¸ì„œë“¤
            question: ì›ë³¸ ì§ˆë¬¸
        Returns:
            bool: ë‹µë³€ì´ ì ì ˆí•œì§€ ì—¬ë¶€
        """
        if not answer or len(answer.strip()) < 20:
            print("Answer too short")
            return False

        # ì»¨í…ìŠ¤íŠ¸ ë¬¸ì„œê°€ ì—†ìœ¼ë©´ RAG ë‹µë³€ìœ¼ë¡œ ë¶€ì ì ˆ
        if not context_docs or len(context_docs) == 0:
            print("No context documents found")
            return False

        # ì¤‘ìš” ë‹¨ì–´ í•„í„°ë§ - ì§ˆë¬¸ì˜ í•µì‹¬ í‚¤ì›Œë“œê°€ ë¬¸ì„œì— ìˆëŠ”ì§€ í™•ì¸
        if question:
            question_lower = question.lower()
            # ì§ˆë¬¸ì—ì„œ ì¤‘ìš”í•œ í‚¤ì›Œë“œ ì¶”ì¶œ (2ê¸€ì ì´ìƒ)
            important_keywords = [word for word in question_lower.split() if len(word) >= 2]
            
            # ë¬¸ì„œ ë‚´ìš©ì— ì¤‘ìš”í•œ í‚¤ì›Œë“œê°€ í¬í•¨ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸
            has_important_keywords = False
            for doc in context_docs:
                doc_content_lower = doc.get('content', '').lower()
                if any(keyword in doc_content_lower for keyword in important_keywords):
                    has_important_keywords = True
                    break
            
            if not has_important_keywords:
                print("No important keywords found in RAG documents")
                return False

        # ë¶€ì •ì ì¸ ì§€í‘œë“¤
        negative_indicators = [
            "ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤",
            "ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤",
            "ì œê³µëœ ìë£Œì—ì„œ",
            "ëª¨ë¥´ê² ìŠµë‹ˆë‹¤",
            "ì•Œ ìˆ˜ ì—†ìŠµë‹ˆë‹¤",
            "cannot find",
            "not found",
            "no information",
            "unable to find"
        ]

        answer_lower = answer.lower()
        for indicator in negative_indicators:
            if indicator in answer_lower:
                print(f"Negative indicator found: {indicator}")
                return False

        # ìœ ì‚¬ë„ ì„ê³„ê°’ì„ 0.75ë¡œ ë‚®ì¶°ì„œ ë” ë§ì€ ë¬¸ì„œë¥¼ í—ˆìš© (RAG ê°€ì¤‘ì¹˜ ì¦ê°€)
        if context_docs and all(doc.get('similarity', 0) < 0.75 for doc in context_docs):
            print("All documents have low similarity")
            return False

        # ê´€ë ¨ì„± ë†’ì€ ë¬¸ì„œê°€ ìˆëŠ”ì§€ í™•ì¸ (ì„ê³„ê°’ ë‚®ì¶¤)
        high_quality_docs = [doc for doc in context_docs if doc.get('similarity', 0) >= 0.75]
        if not high_quality_docs:
            print("No high-quality relevant documents found")
            return False

        print(f"Answer quality check passed. High-quality docs: {len(high_quality_docs)}")
        return True
    
    async def _generate_gpt_fallback(self, question: str, context_docs: List[Dict]) -> str:
        """
        ì˜¨ë³´ë”© êµìœ¡ìš© GPT í´ë°± ë‹µë³€ ìƒì„±
        """
        # ê°„ë‹¨í•œ ì¸ì‚¬ ì²˜ë¦¬
        if any(greeting in question.lower() for greeting in ["ì•ˆë…•", "ì•ˆë…•í•˜ì„¸ìš”", "í•˜ì´", "hi", "hello"]):
            return "ì•ˆë…•í•˜ì„¸ìš”! ğŸ˜Š ì€í–‰ ì‹ ì…ì‚¬ì› ì˜¨ë³´ë”©ì„ ë„ì™€ë“œë¦¬ëŠ” AI íŠœí„°ì…ë‹ˆë‹¤. ìˆ˜ì‹ (ì˜ˆê¸ˆ), ì—¬ì‹ (ëŒ€ì¶œ), ì™¸í™˜(í™˜ì „Â·ì†¡ê¸ˆ) íŒŒíŠ¸ ì¤‘ ê¶ê¸ˆí•œ ì—…ë¬´ê°€ ìˆìœ¼ì‹œë©´ ì–¸ì œë“  ë¬¼ì–´ë³´ì„¸ìš”!"
        
        # ì˜¨ë³´ë”© ê´€ë ¨ ì§ˆë¬¸ì´ì§€ë§Œ RAG ë¬¸ì„œê°€ ì—†ëŠ” ê²½ìš°
        gpt_prompt = f"""ë‹¹ì‹ ì€ ì€í–‰ ì‹ ì…ì‚¬ì› ì˜¨ë³´ë”©ì„ ë‹´ë‹¹í•˜ëŠ” AI íŠœí„°ì…ë‹ˆë‹¤.
ì‹ ì…ì‚¬ì›ì´ íšŒì‚¬ì— ìì—°ìŠ¤ëŸ½ê²Œ ì ì‘í•  ìˆ˜ ìˆë„ë¡ ë„ì™€ì£¼ì„¸ìš”.

ì§ˆë¬¸: {question}

ë‹µë³€ ê·œì¹™:
1. ì‹ ì…ì‚¬ì›ì´ ì´í•´í•˜ê¸° ì‰½ê²Œ ë”°ëœ»í•˜ê³  êµìœ¡ì ì¸ í†¤ìœ¼ë¡œ ë‹µë³€
2. ì–´ë ¤ìš´ ì€í–‰ ìš©ì–´ëŠ” ì‰¬ìš´ í‘œí˜„ê³¼ í•¨ê»˜ ì„¤ëª…
3. ë°˜ë“œì‹œ ë‹¤ìŒ ìˆœì„œë¡œ êµ¬ì„±:
   - â‘  í•µì‹¬ ê°œë… ìš”ì•½ (í•œ ë¬¸ë‹¨)
   - â‘¡ ì‹¤ì œ í˜„ì¥ ì˜ˆì‹œ (êµ¬ì²´ì ì¸ ìƒí™©ì´ë‚˜ ìˆ«ì í¬í•¨)
   - â‘¢ ì‹¤ë¬´ ìœ ì˜ì‚¬í•­ (ì£¼ì˜í•  ì ì´ë‚˜ íŒ)
4. ë‹µë³€ì€ 3-4ë¬¸ë‹¨ ì´ë‚´ë¡œ ê°„ê²°í•˜ê²Œ
5. ì‹ ì…ì‚¬ì›ì´ íšŒì‚¬ì— ìì—°ìŠ¤ëŸ½ê²Œ ì ì‘í•  ìˆ˜ ìˆë„ë¡ ê²©ë ¤í•˜ëŠ” í†¤ ìœ ì§€
6. ë¶ˆí™•ì‹¤í•œ ë‚´ìš©ì€ "ì¼ë°˜ì ìœ¼ë¡œ" ë˜ëŠ” "ë³´í†µ"ì´ë¼ëŠ” í‘œí˜„ ì‚¬ìš©

ë‹µë³€:"""
        
        # GPTë¡œ ë‹µë³€ ìƒì„±
        return self._call_gpt(gpt_prompt)