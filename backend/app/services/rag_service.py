"""
RAG ì„œë¹„ìŠ¤ - ì™„ì „ ìˆ˜ì • ë²„ì „
"""
import os
import json
import asyncio
from typing import List, Dict, Optional
from sqlalchemy.orm import Session
from sqlalchemy import text
import openai
from app.database import get_session

class RAGService:
    def __init__(self, session: Session):
        self.session = session
        self.api_key = os.getenv("OPENAI_API_KEY")
        self.chunk_size = 1000
        self.chunk_overlap = 200
        self.onboarding_keywords = [
            "ì˜¨ë³´ë”©", "ì‹ ì…ì‚¬ì›", "êµìœ¡", "í›ˆë ¨", "ê°€ì´ë“œ", "ë§¤ë‰´ì–¼", "ì ˆì°¨", "í”„ë¡œì„¸ìŠ¤"
        ]
        self.stopwords = ["ì€", "ëŠ”", "ì´", "ê°€", "ì„", "ë¥¼", "ì—", "ì˜", "ë¡œ", "ìœ¼ë¡œ", "ì™€", "ê³¼", "ë„", "ë§Œ", "ë¶€í„°", "ê¹Œì§€", "ì—ì„œ", "ì—ê²Œ", "í•œí…Œ"]

    async def similarity_search(self, query: str, k: int = 5) -> List[Dict]:
        """ìœ ì‚¬ë„ ê²€ìƒ‰ - ì‹ ì…ì‚¬ì› ì˜¨ë³´ë”©ìš© ê°œì„ """
        try:
            print(f"ğŸ” RAG ê²€ìƒ‰ ì‹œì‘: {query}")
            
            # 1. ì œëª© ìš°ì„  ê²€ìƒ‰ (ì •í™•í•œ ë§¤ì¹­)
            title_query = f"""
                SELECT 
                    dc.content,
                    dc.chunk_index,
                    d.title,
                    d.category,
                    d.id as document_id,
                    1.0 as similarity
                FROM document_chunks dc
                JOIN documents d ON dc.document_id = d.id
                WHERE d.is_indexed = true AND d.category = 'RAG'
                AND d.title ILIKE :query
                ORDER BY d.upload_date DESC
                LIMIT :k
            """
            
            result = self.session.execute(
                text(title_query), 
                {"query": f"%{query}%", "k": k}
            ).fetchall()
            
            if result:
                print(f"âœ… ì œëª© ë§¤ì¹­ìœ¼ë¡œ {len(result)}ê°œ ë¬¸ì„œ ë°œê²¬")
                return [
                    {
                        "title": row.title,
                        "content": row.content,
                        "similarity": row.similarity,
                        "document_id": row.document_id,
                        "chunk_index": row.chunk_index
                    }
                    for row in result
                ]
            
            # 2. í‚¤ì›Œë“œë³„ ê°œë³„ ê²€ìƒ‰
            keywords = self._extract_keywords(query)
            print(f"ğŸ” í‚¤ì›Œë“œ ê²€ìƒ‰: {keywords}")
            
            for keyword in keywords:
                result = self.session.execute(
                    text(title_query), 
                    {"query": f"%{keyword}%", "k": k}
                ).fetchall()
                
                if result:
                    print(f"âœ… í‚¤ì›Œë“œ '{keyword}'ë¡œ {len(result)}ê°œ ë¬¸ì„œ ë°œê²¬")
                    return [
                        {
                            "title": row.title,
                            "content": row.content,
                            "similarity": row.similarity,
                            "document_id": row.document_id,
                            "chunk_index": row.chunk_index
                        }
                        for row in result
                    ]
            
            # 3. ì¶”ê°€ í‚¤ì›Œë“œ ê²€ìƒ‰ (ëŒ€ì¶œ ê´€ë ¨)
            if any(word in query.lower() for word in ["ëŒ€ì¶œ", "ìƒí’ˆ", "ì¶”ì²œ", "ìƒë‹´"]):
                loan_keywords = ["ê°€ê³„ëŒ€ì¶œ", "ì£¼íƒë‹´ë³´ëŒ€ì¶œ", "ì „ì›”ì„¸ë³´ì¦ê¸ˆëŒ€ì¶œ", "ê°œì¸ëŒ€ì¶œ", "ì‹ ìš©ëŒ€ì¶œ"]
                for loan_keyword in loan_keywords:
                    result = self.session.execute(
                        text(title_query), 
                        {"query": f"%{loan_keyword}%", "k": k}
                    ).fetchall()
                    
                    if result:
                        print(f"âœ… ëŒ€ì¶œ í‚¤ì›Œë“œ '{loan_keyword}'ë¡œ {len(result)}ê°œ ë¬¸ì„œ ë°œê²¬")
                        return [
                            {
                                "title": row.title,
                                "content": row.content,
                                "similarity": row.similarity,
                                "document_id": row.document_id,
                                "chunk_index": row.chunk_index
                            }
                            for row in result
                        ]
            
            # 4. ë‚´ìš© ê¸°ë°˜ ê²€ìƒ‰ (ë§ˆì§€ë§‰ ì‹œë„)
            content_query = f"""
                SELECT 
                    dc.content,
                    dc.chunk_index,
                    d.title,
                    d.category,
                    d.id as document_id,
                    0.8 as similarity
                FROM document_chunks dc
                JOIN documents d ON dc.document_id = d.id
                WHERE d.is_indexed = true AND d.category = 'RAG'
                AND dc.content ILIKE :query
                ORDER BY d.upload_date DESC
                LIMIT :k
            """
            
            result = self.session.execute(
                text(content_query), 
                {"query": f"%{query}%", "k": k}
            ).fetchall()
            
            if result:
                print(f"âœ… ë‚´ìš© ê²€ìƒ‰ìœ¼ë¡œ {len(result)}ê°œ ë¬¸ì„œ ë°œê²¬")
                return [
                    {
                        "title": row.title,
                    "content": row.content,
                        "similarity": row.similarity,
                        "document_id": row.document_id,
                        "chunk_index": row.chunk_index
                    }
                    for row in result
                ]
            
            print(f"âŒ ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ: {query}")
            return []
            
        except Exception as e:
            print(f"âŒ RAG ê²€ìƒ‰ ì˜¤ë¥˜: {e}")
            return []

    def _extract_keywords(self, question: str) -> List[str]:
        """ì§ˆë¬¸ì—ì„œ í‚¤ì›Œë“œ ì¶”ì¶œ - ì‹ ì…ì‚¬ì› ì˜¨ë³´ë”©ìš©"""
        # ì‹ ì…ì‚¬ì›ì´ ìì£¼ ë¬»ëŠ” í‚¤ì›Œë“œë“¤
        onboarding_keywords = {
            "ëŒ€ì¶œ": ["ëŒ€ì¶œ", "ê°€ê³„ëŒ€ì¶œ", "ì£¼íƒë‹´ë³´ëŒ€ì¶œ", "ì „ì›”ì„¸ë³´ì¦ê¸ˆëŒ€ì¶œ", "ê¸°ì—…ëŒ€ì¶œ", "ëŒ€í™˜ëŒ€ì¶œ", "ì‹ ìš©ëŒ€ì¶œ"],
            "ìƒí’ˆ": ["ìƒí’ˆ", "ìƒí’ˆì„¤ëª…ì„œ", "ìƒí’ˆì•ˆë‚´"],
            "ê³ ê°": ["ê³ ê°", "70ëŒ€", "ì—°ë ¹", "ë‚˜ì´"],
            "ì¶”ì²œ": ["ì¶”ì²œ", "ìƒë‹´", "ë¬¸ì˜"],
            "ê¸ˆë¦¬": ["ê¸ˆë¦¬", "ì´ì", "ìˆ˜ìˆ˜ë£Œ"],
            "ì•½ê´€": ["ì•½ê´€", "ê¸°ë³¸ì•½ê´€", "íŠ¹ì•½", "ì´ìš©ì•½ê´€"],
            "ì–‘ì‹": ["ì–‘ì‹", "ì„œì‹", "ì‹ ì²­ì„œ", "ìœ„ì„ì¥", "í™•ì¸ì„œ", "í•´ì´‰ì¦ëª…ì„œ", "ì´ì˜ì‹ ì²­ì„œ"],
            "ì¦ëª…ì„œ": ["ì¦ëª…ì„œ", "í•´ì´‰ì¦ëª…ì„œ", "ì†Œë“ì¦ëª…ì„œ", "ì¬ì§ì¦ëª…ì„œ"],
            "ì‹ ì²­ì„œ": ["ì‹ ì²­ì„œ", "ì´ì˜ì‹ ì²­ì„œ", "ëŒ€ì¶œì‹ ì²­ì„œ", "ê³„ì¢Œì‹ ì²­ì„œ", "í”¼í•´êµ¬ì œì‹ ì²­ì„œ"],
            "ì•½ì •ì„œ": ["ì•½ì •ì„œ", "ì‹ ìš©ë³´ì¦ì•½ì •ì„œ", "ëŒ€ì¶œì•½ì •ì„œ", "ëŒ€ì¶œê±°ë˜ì•½ì •ì„œ"],
            "ë™ì˜ì„œ": ["ë™ì˜ì„œ", "ê°œì¸ì •ë³´ë™ì˜ì„œ", "ì œ3ìì œê³µë™ì˜ì„œ", "ê°œì¸ì •ë³´ìˆ˜ì§‘ë™ì˜ì„œ"],
            "ì•ˆë‚´": ["ì•ˆë‚´", "ê³ ê°ê¶Œë¦¬ì•ˆë‚´ë¬¸", "ëŒ€ì¶œë§Œê¸°ì•ˆë‚´", "ëŒ€ì¶œë§Œê¸°ê²½ê³¼ì•ˆë‚´"],
            "ìƒí™©í‘œ": ["ìƒí™©í‘œ", "ìˆ˜ì‹ ê±°ë˜ìƒí™©í‘œ", "ì—¬ì‹ ê±°ë˜ìƒí™©í‘œ"],
            "ê³„ì¢Œ": ["ê³„ì¢Œ", "ì¦ê¶Œê³„ì¢Œ", "ê³„ì¢Œê°œì„¤", "ì¦ê¶Œê³„ì¢Œê°œì„¤", "ê³„ì¢Œì‹ ì²­", "ê³„ì¢Œí†µí•©ê´€ë¦¬"],
            "ì¦ê¶Œ": ["ì¦ê¶Œ", "ì¦ê¶Œê³„ì¢Œ", "ì¦ê¶Œê³„ì¢Œê°œì„¤", "ì¦ê¶Œì„œë¹„ìŠ¤", "ì¦ê¶Œê³„ì¢Œê°œì„¤ì„œë¹„ìŠ¤"],
            "ì˜ˆê¸ˆ": ["ì˜ˆê¸ˆ", "ì…ì¶œê¸ˆì´ììœ ë¡œìš´ì˜ˆê¸ˆ", "ì ë¦½ì‹ì˜ˆê¸ˆ", "ê±°ì¹˜ì‹ì˜ˆê¸ˆ", "ì™¸í™”ì˜ˆê¸ˆ"],
            "ì „ìê¸ˆìœµ": ["ì „ìê¸ˆìœµ", "ì „ìê¸ˆìœµê±°ë˜", "ëª¨ë°”ì¼ë±…í‚¹", "ì´ì²´í•œë„", "ì˜¤í”ˆë±…í‚¹"],
            "ë³´ì´ìŠ¤í”¼ì‹±": ["ë³´ì´ìŠ¤í”¼ì‹±", "í”¼í•´êµ¬ì œ", "ì „ê¸°í†µì‹ ê¸ˆìœµì‚¬ê¸°", "í”¼í•´êµ¬ì œì‹ ì²­ì„œ"],
            "ì‹ ìš©ë³´ì¦": ["ì‹ ìš©ë³´ì¦", "ì‹ ìš©ë³´ì¦ì•½ì •ì„œ", "ì‹ ìš©ë³´ì¦ì„œ", "ë³´ì¦ì±„ë¬´ì´í–‰ì²­êµ¬ì„œ"],
            "ì²´í¬ì¹´ë“œ": ["ì²´í¬ì¹´ë“œ", "êµí†µì¹´ë“œ", "í›„ë¶ˆêµí†µì¹´ë“œ"],
            "ëª¨ì„í†µì¥": ["ëª¨ì„í†µì¥", "ëª¨ì„í†µì¥ì„œë¹„ìŠ¤"],
            "í–‡ì‚´ë¡ ": ["í–‡ì‚´ë¡ ", "í–‡ì‚´ë¡ ë±…í¬"],
            "ì „ì„¸ì§€í‚´": ["ì „ì„¸ì§€í‚´", "ì „ì„¸ì§€í‚´ë³´ì¦ì•½ê´€"]
        }
        
        keywords = []
        question_lower = question.lower()
        
        # ì§ˆë¬¸ì—ì„œ í‚¤ì›Œë“œ ì°¾ê¸°
        for category, words in onboarding_keywords.items():
            for word in words:
                if word in question_lower:
                    keywords.extend(words)
                    break
        
        # ì¤‘ë³µ ì œê±°í•˜ê³  ìƒìœ„ 8ê°œë§Œ ë°˜í™˜ (ë” ë§ì€ í‚¤ì›Œë“œë¡œ ê²€ìƒ‰)
        unique_keywords = list(set(keywords))
        print(f"ğŸ” ì¶”ì¶œëœ í‚¤ì›Œë“œ: {unique_keywords}")
        return unique_keywords[:8]
    
    def _call_gpt(self, prompt: str) -> str:
        """GPT API ì§ì ‘ í˜¸ì¶œ"""
        try:
            headers = {
                "Authorization": f"Bearer {self.api_key}",
                "Content-Type": "application/json"
            }
            
            data = {
                "model": "gpt-3.5-turbo",
                "messages": [
                    {"role": "system", "content": "ë‹¹ì‹ ì€ ì€í–‰ ì˜¨ë³´ë”© ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤. ğŸ»"},
                    {"role": "user", "content": prompt}
                ],
                "max_tokens": 1000,
                "temperature": 0.7
            }
            
            import requests
            response = requests.post(
                "https://api.openai.com/v1/chat/completions",
                headers=headers,
                json=data,
                timeout=30
            )
            
            if response.status_code == 200:
                result = response.json()
                return result["choices"][0]["message"]["content"].strip()
            else:
                print(f"OpenAI API error: {response.status_code} - {response.text}")
                return "ì£„ì†¡í•©ë‹ˆë‹¤. ì¼ì‹œì ì¸ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."
                
        except Exception as e:
            print(f"GPT API call error: {e}")
            return "ì£„ì†¡í•©ë‹ˆë‹¤. ì¼ì‹œì ì¸ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."

    async def generate_rag_answer(self, question: str) -> Dict:
        """RAG ë‹µë³€ ìƒì„± ë©”ì„œë“œ - ì™„ì „ ìˆ˜ì •"""
        try:
            # ìœ ì‚¬ë„ ê²€ìƒ‰ìœ¼ë¡œ ê´€ë ¨ ë¬¸ì„œ ì°¾ê¸°
            similar_docs = await self.similarity_search(question, k=5)
            print(f"RAG ê²€ìƒ‰ ê²°ê³¼: {len(similar_docs)}ê°œ ë¬¸ì„œ ë°œê²¬")
            
            if not similar_docs:
                # ê´€ë ¨ ë¬¸ì„œê°€ ì—†ìœ¼ë©´ ì¼ë°˜ GPT ë‹µë³€
                print("ê´€ë ¨ ë¬¸ì„œ ì—†ìŒ - ì¼ë°˜ GPT ë‹µë³€ ìƒì„±")
                answer = self._call_gpt(f"ì§ˆë¬¸: {question}\n\nì€í–‰ ì—…ë¬´ì— ê´€ë ¨ëœ ë‹µë³€ì„ í•´ì£¼ì„¸ìš”.")
                return {
                    "answer": answer,
                    "sources": [],
                    "response_time": 0.0
                }
            
            # ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±
            context = "\n\n".join([
                f"[{doc['title']}]\n{doc['content']}"
                for doc in similar_docs
            ])
            
            # AI í•˜ë¦¬ë³´ ì‹ ì…ì‚¬ì› ì˜¨ë³´ë”© í”„ë¡¬í”„íŠ¸
            prompt = f"""
ë‹¹ì‹ ì€ AI í•˜ë¦¬ë³´ì…ë‹ˆë‹¤. ğŸ» ì‹ ì…ì‚¬ì› ì˜¨ë³´ë”©ì„ ë„ì™€ì£¼ëŠ” ì¹œê·¼í•œ ì€í–‰ ì–´ì‹œìŠ¤í„´íŠ¸ì˜ˆìš”.

ë‹¤ìŒ ê²€ìƒ‰ ì»¨í…ìŠ¤íŠ¸ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ë‹µë³€í•˜ì„¸ìš”:

{context}

ì§ˆë¬¸: {question}

ğŸ¯ AI í•˜ë¦¬ë³´ ë‹µë³€ ê°€ì´ë“œë¼ì¸:
- **ì‹ ì…ì‚¬ì›ì´ ê³ ê° ìƒë‹´í•  ë•Œ ë°”ë¡œ í™œìš©í•  ìˆ˜ ìˆëŠ” ì‹¤ë¬´ ì •ë³´ë¥¼ ì œê³µí•˜ì„¸ìš”**
- **ì‹ ì…ì‚¬ì›ì—ê²Œ ì¡°ì–¸í•˜ëŠ” í†¤ìœ¼ë¡œ ë‹µë³€í•˜ì„¸ìš” (ê³ ê°ì—ê²Œ ì§ì ‘ ë§í•˜ëŠ” í†¤ì´ ì•„ë‹˜)**
- ìì—°ìŠ¤ëŸ¬ìš´ ë¬¸ë‹¨ìœ¼ë¡œ êµ¬ì„±í•˜ì—¬ ê¸¸ê³  ìƒì„¸í•˜ê²Œ ì„¤ëª…í•˜ì„¸ìš” (ë¶ˆë¦¿ í¬ì¸íŠ¸ë‚˜ ë‹¨ê³„ë³„ ë‚˜ì—´ ê¸ˆì§€)
- í•µì‹¬ ì •ë³´ë¥¼ ë¨¼ì € ì œì‹œí•˜ê³ , ì„¸ë¶€ì‚¬í•­ì„ í¬í•¨í•œ ì™„ì „í•œ ì„¤ëª…ì„ ì œê³µí•˜ì„¸ìš”
- ë¬¸ë‹¨ê³¼ ë¬¸ë‹¨ ì‚¬ì´ëŠ” ìì—°ìŠ¤ëŸ½ê²Œ ì—°ê²°í•˜ì—¬ íë¦„ ìˆê²Œ ì‘ì„±í•˜ì„¸ìš”
- í•œêµ­ì–´ë¡œ ì¹œê·¼í•˜ê²Œ ë‹µë³€í•˜ì„¸ìš” (ğŸ» ì´ëª¨í‹°ì½˜ì€ ë‹µë³€ ì‹œì‘ì—ë§Œ ì‚¬ìš©)
- AI í•˜ë¦¬ë³´ë¡œì„œ ì‹ ì…ì‚¬ì›ì„ ë„ì™€ì£¼ëŠ” ë§ˆìŒìœ¼ë¡œ ë‹µë³€í•˜ì„¸ìš”
- ë¬¸ì„œ ë‚´ìš©ì´ ìˆìœ¼ë©´ ë°˜ë“œì‹œ ë¬¸ì„œ ë‚´ìš©ì„ ìš°ì„  í™œìš©í•˜ê³ , ë¬¸ì„œ ë‚´ìš©ì— ì—†ëŠ” ë¶€ë¶„ë§Œ ì¼ë°˜ì ì¸ ì€í–‰ ì—…ë¬´ ì§€ì‹ìœ¼ë¡œ ë³´ì™„í•˜ì„¸ìš”
- ì§ˆë¬¸ê³¼ ê´€ë ¨ ì—†ëŠ” ë‚´ìš©ì€ ë‹µë³€ì— í¬í•¨í•˜ì§€ ë§ˆì„¸ìš”
- ì§ˆë¬¸ì˜ í•µì‹¬ í‚¤ì›Œë“œì™€ ì§ì ‘ ê´€ë ¨ëœ ë‚´ìš©ë§Œ ë‹µë³€í•˜ì„¸ìš”
- ë¬¸ì¥ì„ ì¶©ë¶„íˆ ê¸¸ê²Œ ì‘ì„±í•˜ì—¬ ìƒì„¸í•œ ì„¤ëª…ì„ ì œê³µí•˜ì„¸ìš”
- ê° ë¬¸ë‹¨ì€ 5-7ë¬¸ì¥ìœ¼ë¡œ êµ¬ì„±í•˜ì—¬ ì¶©ë¶„í•œ ì •ë³´ë¥¼ ì œê³µí•˜ì„¸ìš”

ğŸ¦ ìƒí’ˆ ì¶”ì²œ ê´€ë ¨ ì§ˆë¬¸:
- êµ¬ì²´ì ì¸ ìƒí’ˆëª…ê³¼ íŠ¹ì§•ì„ í¬í•¨í•˜ì—¬ ë‹µë³€í•˜ì„¸ìš”
- ì—°ë ¹ëŒ€ë³„ ê³ ê° íŠ¹ì„±ì„ ê³ ë ¤í•œ ë§ì¶¤í˜• ìƒí’ˆ ì¶”ì²œì„ ì œê³µí•˜ì„¸ìš”
- ëŒ€ì¶œ ìƒí’ˆì˜ ê²½ìš° ê¸ˆë¦¬, í•œë„, ìƒí™˜ì¡°ê±´ ë“± êµ¬ì²´ì ì¸ ì •ë³´ë¥¼ í¬í•¨í•˜ì„¸ìš”
- ëŒ€ì¶œ ìƒë‹´ ì§ˆë¬¸ì—ëŠ” ì‹¤ì œ ëŒ€ì¶œ ìƒí’ˆì„ ì¶”ì²œí•˜ê³ , ì•Œë¦¼ ì„œë¹„ìŠ¤ë‚˜ ê¸°íƒ€ ì„œë¹„ìŠ¤ëŠ” ì¶”ì²œí•˜ì§€ ë§ˆì„¸ìš”
- ëŒ€ì¶œ ìƒí’ˆ ì¶”ì²œ ì‹œì—ëŠ” êµ¬ì²´ì ì¸ ìƒí’ˆëª…(ê°€ê³„ëŒ€ì¶œ, ì£¼íƒë‹´ë³´ëŒ€ì¶œ, ì „ì›”ì„¸ë³´ì¦ê¸ˆëŒ€ì¶œ ë“±)ì„ ëª…ì‹œí•˜ì„¸ìš”

ğŸš¨ **70ëŒ€ ê³ ê° ëŒ€ì¶œ ìƒë‹´ ì‹œ í•„ìˆ˜ ê·œì¹™:**
- **ë°˜ë“œì‹œ ê°œì¸ ëŒ€ì¶œ ìƒí’ˆë§Œ ì¶”ì²œ: ê°€ê³„ëŒ€ì¶œ, ì£¼íƒë‹´ë³´ëŒ€ì¶œ, ì „ì›”ì„¸ë³´ì¦ê¸ˆëŒ€ì¶œ, ê°œì¸ëŒ€ì¶œ, ì‹ ìš©ëŒ€ì¶œ**
- **ì ˆëŒ€ ê¸ˆì§€: ê¸°ì—…ëŒ€ì¶œ, ì‚¬ì¥ë‹˜ëŒ€í™˜ëŒ€ì¶œ, ëª¨ë°”ì¼ìš°ëŒ€ë³´ì¦ëŒ€ì¶œ, ì‚¬ì—…ìëŒ€ì¶œ, ë³´ì¦ëŒ€ì¶œ, í–‡ì‚´ë¡  ë“± ëª¨ë“  ê¸°ì—…/ì‚¬ì—…ììš© ìƒí’ˆ**
- **70ëŒ€ = ê°œì¸ ê³ ê°ìœ¼ë¡œ ê°„ì£¼í•˜ê³  ê°œì¸ ëŒ€ì¶œ ìƒí’ˆë§Œ ì¶”ì²œ**
- **ì‚¬ì—… ìš´ì˜ ì—¬ë¶€ì™€ ê´€ê³„ì—†ì´ 70ëŒ€ ê³ ê°ì—ê²ŒëŠ” ê°œì¸ ëŒ€ì¶œ ìƒí’ˆë§Œ ì¶”ì²œ**
- **70ëŒ€ ê³ ê°ì—ê²ŒëŠ” ê°€ê³„ëŒ€ì¶œì„ ìš°ì„ ì ìœ¼ë¡œ ì¶”ì²œí•˜ê³ , ìƒì„¸í•œ íŠ¹ì§•ê³¼ ì¥ì ì„ ì„¤ëª…í•˜ì„¸ìš”**

- ëŒ€ì¶œ ìƒë‹´ ì§ˆë¬¸ì—ëŠ” ë°˜ë“œì‹œ êµ¬ì²´ì ì¸ ê°œì¸ ëŒ€ì¶œ ìƒí’ˆëª…ì„ ì¶”ì²œí•˜ê³ , ë‹¨ê³„ë³„ ì„¤ëª…ì´ë‚˜ ì¼ë°˜ì ì¸ ì¡°ì–¸ì€ í”¼í•˜ì„¸ìš”
- ê³ ê°ì—ê²Œ ì§ì ‘ì ì¸ ê°œì¸ ëŒ€ì¶œ ìƒí’ˆ ì¶”ì²œì„ ì œê³µí•˜ì„¸ìš”

ğŸ“‹ ì–‘ì‹/ì„œë¥˜ ê´€ë ¨ ì§ˆë¬¸:
- í•´ì´‰ì¦ëª…ì„œ, ì´ì˜ì‹ ì²­ì„œ, ìœ„ì„ì¥ ë“± êµ¬ì²´ì ì¸ ì–‘ì‹ëª…ì„ ëª…ì‹œí•˜ì„¸ìš”
- ì‹ ì²­ ì ˆì°¨ì™€ í•„ìš”í•œ ì„œë¥˜ë¥¼ ìƒì„¸íˆ ì•ˆë‚´í•˜ì„¸ìš”
- ì‹ ì…ì‚¬ì›ì´ ê³ ê°ì—ê²Œ ì„¤ëª…í•  ìˆ˜ ìˆëŠ” ìˆ˜ì¤€ìœ¼ë¡œ ì‘ì„±í•˜ì„¸ìš”

âš ï¸ ì£¼ì˜ì‚¬í•­:
- ê³ ê°ì„¼í„° ì „í™”ë²ˆí˜¸ë‚˜ ì—°ë½ì²˜ëŠ” ì ˆëŒ€ í¬í•¨í•˜ì§€ ë§ˆì„¸ìš”
- ì‹ ì…ì‚¬ì›ì´ ê³ ê° ìƒë‹´ ì‹œ ì°¸ê³ í•  ìˆ˜ ìˆëŠ” ì‹¤ë¬´ ì •ë³´ë¥¼ ì œê³µí•˜ì„¸ìš”
- ì§ˆë¬¸ê³¼ ê´€ë ¨ ì—†ëŠ” ìƒí’ˆ(ì˜ˆ: ì•„ì´ í†µì¥ì„ ë…¸ì¸ ëŒ€ì¶œ ìƒë‹´ì—ì„œ ì–¸ê¸‰)ì€ ì ˆëŒ€ ì¶”ì²œí•˜ì§€ ë§ˆì„¸ìš”
- **ì¤‘ìš”: ë‹µë³€ì—ì„œ "í† ìŠ¤ë±…í¬"ë¼ëŠ” ë‹¨ì–´ê°€ ë‚˜ì˜¤ë©´ ë°˜ë“œì‹œ "í•˜ê²½ì€í–‰"ìœ¼ë¡œ ë°”ê¿”ì„œ ë‹µë³€í•˜ì„¸ìš”**

ë‹µë³€:
"""
            
            answer = self._call_gpt(prompt)
            
            # í† ìŠ¤ë±…í¬ë¥¼ í•˜ê²½ì€í–‰ìœ¼ë¡œ ë³€ê²½
            answer = answer.replace("í† ìŠ¤ë±…í¬", "í•˜ê²½ì€í–‰")
            
            # ì°¸ê³ ìë£Œ êµ¬ì„± - ì„ì‹œë¡œ ëª¨ë“  ë¬¸ì„œ í¬í•¨ (ë””ë²„ê¹…ìš©)
            sources = []
            for doc in similar_docs:
                sources.append({
                    "title": doc["title"],
                    "content": doc["content"][:200] + "..." if len(doc["content"]) > 200 else doc["content"]
                })
            
            # ì°¸ê³ ìë£Œë¥¼ ë‹µë³€ì— ì¶”ê°€ (ì¤‘ë³µ ì œê±°)
            if sources:
                # ì¤‘ë³µ ì œê±° (title ê¸°ì¤€)
                unique_sources = []
                seen_titles = set()
                for source in sources:
                    if source['title'] not in seen_titles:
                        unique_sources.append(source)
                        seen_titles.add(source['title'])
                
                if unique_sources:
                    answer += "\n\nì°¸ê³  ìë£Œ:\n"
                    for source in unique_sources:
                        answer += f"\nâ€¢ {source['title']}"
                
                # sourcesë¥¼ unique_sourcesë¡œ ì—…ë°ì´íŠ¸
                sources = unique_sources
            
            return {
                "answer": answer,
                "sources": sources,
                "response_time": 0.0
            }
            
        except Exception as e:
            print(f"Generate RAG answer error: {e}")
            return {
                "answer": "ì•—, ì ê¹ë§Œìš”! ğŸ»\nì¼ì‹œì ì¸ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆì–´ìš”.\nì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.",
                "sources": [],
                "response_time": 0.0
            }

    async def process_query(self, question: str) -> Dict:
        """ì¿¼ë¦¬ ì²˜ë¦¬ ë©”ì¸ ë©”ì„œë“œ"""
        try:
            # RAG ë‹µë³€ ìƒì„±
            result = await self.generate_rag_answer(question)
            
            return {
                "answer": result["answer"],
                "sources": result.get("sources", []),
                "response_time": result.get("response_time", 0.0)
            }
            
        except Exception as e:
            print(f"Process query error: {e}")
            return {
                "answer": "ì•—, ì ê¹ë§Œìš”! ğŸ»\nì¼ì‹œì ì¸ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆì–´ìš”.\nì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.",
                "sources": [],
                "response_time": 0.0
            }
