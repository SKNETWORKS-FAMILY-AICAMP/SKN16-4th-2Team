"""
RAG (Retrieval-Augmented Generation) ì±—ë´‡ ì„œë¹„ìŠ¤
ê°„ë‹¨í•œ OpenAI API ì§ì ‘ í˜¸ì¶œ ë°©ì‹
"""
from typing import List, Dict, Optional
import requests
import json
from datetime import datetime
from sqlmodel import Session, select
from sqlalchemy import text

from app.config import settings
from app.models.document import Document, DocumentChunk
from app.models.mentor import ChatHistory
from .title_lexicon import LEXICON, title_candidates, DOC_TYPE_PRIORITIES

class RAGService:
    """RAG ì±—ë´‡ ì„œë¹„ìŠ¤ í´ë˜ìŠ¤"""
    
    def __init__(self, session: Session):
        self.session = session
        self.api_key = settings.OPENAI_API_KEY  # í™˜ê²½ë³€ìˆ˜ì—ì„œ ë¡œë“œ
        self.base_url = "https://api.openai.com/v1"
        
        # í…ìŠ¤íŠ¸ ë¶„í•  ì„¤ì •
        self.chunk_size = 1000
        self.chunk_overlap = 200
        
        # ì˜¨ë³´ë”© ê´€ë ¨ í‚¤ì›Œë“œ (ê¸ì •)
        self.onboarding_keywords = [
            "ì¸ì‚¬", "ê·¼íƒœ", "ë³µë¦¬í›„ìƒ", "ì—°ì°¨", "ì‹œìŠ¤í…œ", "ê²°ì¬", "ê³„ì¢Œ", "ì „ì‚°", "ë³´ì•ˆ", "ì½”ë“œ", "êµìœ¡",
            "ì€í–‰", "ì—…ë¬´", "ê·œì •", "ëŒ€ì¶œ", "ì˜ˆê¸ˆ", "ì ê¸ˆ", "ì™¸í™˜", "ì†¡ê¸ˆ", "í™˜ì „", "ê³ ê°", "ì°½êµ¬",
            "ë§¤ë‰´ì–¼", "ì ˆì°¨", "ë„¤íŠ¸ì›Œí¬", "ë°ì´í„°", "ë³´ê³ ì„œ", "íšŒì˜", "ë¯¸íŒ…", "í”„ë¡œì íŠ¸", "íŒ€", "ë¶€ì„œ", "ì¡°ì§"
        ]
        
        # ë¶ˆìš©ì–´ (ë¶€ì •) - ì •ë§ ì“¸ë°ì—†ëŠ” ì§ˆë¬¸ë“¤ë§Œ
        self.stopwords = [
            "ë°¥", "ì‹ì‚¬", "ë¨¹ì—ˆ", "ë°°ê³ íŒŒ", "ë°°ë¶ˆëŸ¬", "ë‚ ì”¨", "ë¹„", "ëˆˆ", "ë§‘", "íë¦¼",
            "ì‹¬ì‹¬", "ì¬ë¯¸", "ë†€", "ì˜í™”", "ë“œë¼ë§ˆ", "ë…¸ë˜", "ìŒì•…", "ê²Œì„", "ì¶•êµ¬", "ì•¼êµ¬",
            "ì‚¬ë‘", "ì¢‹ì•„í•´", "ì‹«ì–´í•´", "ì—¬ìì¹œêµ¬", "ë‚¨ìì¹œêµ¬", "ì—°ì• ", "ê²°í˜¼", "ì•„ì´",
            "ì£¼ì‹", "íˆ¬ì", "ë¡œë˜", "ë³µê¶Œ", "ë„ë°•", "ìˆ ", "ë‹´ë°°", "ì·¨ë¯¸", "ì—¬í–‰",
            "ì‡¼í•‘", "ì˜·", "í™”ì¥", "ìš´ë™", "í—¬ìŠ¤", "ë‹¤ì´ì–´íŠ¸", "ê±´ê°•", "ë³‘", "ì•½", "ì˜ì‚¬",
            "ì•„ì´ëŒ", "ì—°ì˜ˆì¸", "ë°°ìš°", "ê°€ìˆ˜", "ì •ì¹˜", "ì„ ê±°", "ë‰´ìŠ¤", "ì‹œì‚¬"
        ]
    
    def _is_onboarding_related(self, query: str) -> bool:
        """
        ì§ˆë¬¸ì´ ì˜¨ë³´ë”© ê´€ë ¨ì¸ì§€ ì‚¬ì „ í•„í„°ë§ (ë¼ì´íŠ¸ ë²„ì „)
        """
        query_lower = query.lower().strip()
        
        # ê°„ë‹¨í•œ ì¸ì‚¬ëŠ” ë¨¼ì € í—ˆìš© (ì˜¨ë³´ë”© íŠœí„°ë¡œì„œ ì •ì¤‘í•œ ì‘ë‹µ)
        greetings = ["ì•ˆë…•", "ì•ˆë…•í•˜ì„¸ìš”", "í•˜ì´", "hi", "hello", "ë°˜ê°‘"]
        if any(greeting in query_lower for greeting in greetings):
            return True
        
        # ì˜¨ë³´ë”© ê´€ë ¨ í‚¤ì›Œë“œ í¬í•¨ ì—¬ë¶€ í™•ì¸ (ì§§ì€ ì§ˆë¬¸ë„ í—ˆìš©)
        for keyword in self.onboarding_keywords:
            if keyword in query_lower:
                return True
        
        # ëª…ë°±í•œ ë¶ˆìš©ì–´ë§Œ í•„í„°ë§ (ì •í™•í•œ ë‹¨ì–´ ë§¤ì¹­)
        explicit_stopwords = ["ë°¥", "ì‹ì‚¬", "ë¨¹ì—ˆ", "ë‚ ì”¨", "ë¹„", "ëˆˆ", "ì˜í™”", "ë“œë¼ë§ˆ", 
                             "ì‚¬ë‘", "ì—¬ìì¹œêµ¬", "ë‚¨ìì¹œêµ¬", "ì£¼ì‹", "íˆ¬ì", "ë¡œë˜", 
                             "ì•„ì´ëŒ", "ì—°ì˜ˆì¸", "ì •ì¹˜", "ì„ ê±°", "ë‰´ìŠ¤"]
        
        # ì •í™•í•œ ë‹¨ì–´ ë§¤ì¹­ìœ¼ë¡œë§Œ í•„í„°ë§
        for stopword in explicit_stopwords:
            if stopword == query_lower.strip():
                return False
        
        # ë‚˜ë¨¸ì§€ëŠ” ëª¨ë‘ í—ˆìš© (GPTê°€ ì˜¨ë³´ë”© ê´€ë ¨ ì—¬ë¶€ íŒë‹¨)
        return True
    
    def _split_text(self, text: str) -> List[str]:
        """í…ìŠ¤íŠ¸ë¥¼ ì²­í¬ë¡œ ë¶„í• """
        chunks = []
        start = 0
        
        while start < len(text):
            end = start + self.chunk_size
            
            # ë§ˆì§€ë§‰ ì²­í¬ê°€ ì•„ë‹ˆê³  ì¤‘ë³µì´ í•„ìš”í•œ ê²½ìš°
            if end < len(text) and self.chunk_overlap > 0:
                # ë‹¨ì–´ ê²½ê³„ì—ì„œ ìë¥´ê¸°
                while end > start and text[end] not in ' \n\t':
                    end -= 1
                if end == start:  # ë‹¨ì–´ê°€ ë„ˆë¬´ ê¸´ ê²½ìš°
                    end = start + self.chunk_size
            
            chunk = text[start:end].strip()
            if chunk:
                chunks.append(chunk)
            
            # ë‹¤ìŒ ì²­í¬ ì‹œì‘ì  (ì¤‘ë³µ ì œì™¸)
            start = end - self.chunk_overlap if end < len(text) else end
        
        return chunks
    
    def _get_embedding(self, text: str) -> List[float]:
        """í…ìŠ¤íŠ¸ì˜ ì„ë² ë”© ë²¡í„° ìƒì„±"""
        try:
            headers = {
                "Authorization": f"Bearer {self.api_key}",
                "Content-Type": "application/json"
            }
            
            data = {
                "model": "text-embedding-ada-002",
                "input": text
            }
            
            response = requests.post(
                f"{self.base_url}/embeddings",
                headers=headers,
                json=data,
                timeout=30
            )
            
            if response.status_code == 200:
                result = response.json()
                return result["data"][0]["embedding"]
            else:
                print(f"Embedding API error: {response.status_code} - {response.text}")
                return []
                
        except Exception as e:
            print(f"Embedding error: {e}")
            return []

    async def index_document(self, document_id: int, content: str) -> bool:
        """
        ë¬¸ì„œë¥¼ ì²­í¬ë¡œ ë¶„í• í•˜ê³  ì„ë² ë”© ìƒì„±
        Args:
            document_id: ë¬¸ì„œ ID
            content: ë¬¸ì„œ ë‚´ìš©
        Returns:
            bool: ì„±ê³µ ì—¬ë¶€
        """
        try:
            # í…ìŠ¤íŠ¸ ë¶„í• 
            chunks = self._split_text(content)
            
            # ê¸°ì¡´ ì²­í¬ ì‚­ì œ
            existing_chunks = self.session.exec(
                select(DocumentChunk).where(DocumentChunk.document_id == document_id)
            ).all()
            for chunk in existing_chunks:
                self.session.delete(chunk)
            
            # ìƒˆ ì²­í¬ ìƒì„± ë° ì„ë² ë”©
            for i, chunk_text in enumerate(chunks):
                embedding = self._get_embedding(chunk_text)
                if embedding:
                    chunk = DocumentChunk(
                        document_id=document_id,
                        chunk_index=i,
                        content=chunk_text,
                        embedding=embedding
                    )
                    self.session.add(chunk)
            
            # ë¬¸ì„œ ì¸ë±ì‹± ìƒíƒœ ì—…ë°ì´íŠ¸
            document = self.session.get(Document, document_id)
            if document:
                document.is_indexed = True
            
            self.session.commit()
            return True
            
        except Exception as e:
            print(f"Indexing error: {e}")
            self.session.rollback()
            return False

    async def similarity_search(self, query: str, k: int = 5) -> List[Dict]:
        """
        ìœ ì‚¬ë„ ê²€ìƒ‰ìœ¼ë¡œ ê´€ë ¨ ë¬¸ì„œ ì²­í¬ ì°¾ê¸°
        Args:
            query: ê²€ìƒ‰ ì¿¼ë¦¬
            k: ë°˜í™˜í•  ê²°ê³¼ ìˆ˜
        Returns:
            List[Dict]: ê´€ë ¨ ë¬¸ì„œ ì²­í¬ë“¤
        """
        try:
            # ê°„ë‹¨í•œ í…ìŠ¤íŠ¸ ê²€ìƒ‰ (ë²¡í„° ê²€ìƒ‰ ì™„ì „ ì œê±°)
            sql = text("""
                SELECT 
                    dc.content,
                    dc.chunk_index,
                    d.title,
                    d.category,
                    d.id as document_id,
                    0.9 as similarity
                FROM document_chunks dc
                JOIN documents d ON dc.document_id = d.id
                WHERE d.is_indexed = true AND d.category = 'RAG'
                AND (dc.content ILIKE :query OR d.title ILIKE :query)
                ORDER BY d.upload_date DESC
                LIMIT :k
            """)
            
            print(f"í…ìŠ¤íŠ¸ ê²€ìƒ‰ ì¿¼ë¦¬: {query}")
            
            result = self.session.execute(sql, {"query": f"%{query}%", "k": k})
            
            results = []
            for row in result:
                results.append({
                    "content": row.content,
                    "title": row.title,
                    "category": row.category,
                    "document_id": row.document_id,
                    "chunk_index": row.chunk_index,
                    "similarity": float(row.similarity)
                })
            
            print(f"Found {len(results)} relevant chunks for query: {query[:50]}...")
            return results
            
        except Exception as e:
            print(f"Similarity search error: {e}")
            # íŠ¸ëœì­ì…˜ ë¡¤ë°± ë° ìƒˆ ì„¸ì…˜ ì‹œì‘
            try:
                self.session.rollback()
                self.session.close()
                # ìƒˆ ì„¸ì…˜ ìƒì„±
                from app.database import get_session
                self.session = next(get_session())
            except Exception as rollback_error:
                print(f"Rollback error: {rollback_error}")
                # ì™„ì „íˆ ìƒˆ ì„¸ì…˜ìœ¼ë¡œ êµì²´
                try:
                    from app.database import get_session
                    self.session = next(get_session())
                except Exception:
                pass
            return []
    
    def _extract_keywords(self, question: str) -> List[str]:
        """ì§ˆë¬¸ì—ì„œ í‚¤ì›Œë“œ ì¶”ì¶œ"""
        # ëŒ€ì¶œ ê´€ë ¨ í‚¤ì›Œë“œ ë§¤í•‘
        loan_keywords = {
            "ëŒ€ì¶œ": ["ëŒ€ì¶œ", "ê°€ê³„ëŒ€ì¶œ", "ì£¼íƒë‹´ë³´ëŒ€ì¶œ", "ì „ì›”ì„¸ë³´ì¦ê¸ˆëŒ€ì¶œ", "ê¸°ì—…ëŒ€ì¶œ"],
            "ìƒí’ˆ": ["ìƒí’ˆ", "ìƒí’ˆì„¤ëª…ì„œ", "ìƒí’ˆì•ˆë‚´"],
            "ê³ ê°": ["ê³ ê°", "70ëŒ€", "ì—°ë ¹", "ë‚˜ì´"],
            "ì¶”ì²œ": ["ì¶”ì²œ", "ìƒë‹´", "ë¬¸ì˜"],
            "ê¸ˆë¦¬": ["ê¸ˆë¦¬", "ì´ì", "ìˆ˜ìˆ˜ë£Œ"],
            "ì•½ê´€": ["ì•½ê´€", "ê¸°ë³¸ì•½ê´€", "íŠ¹ì•½"],
            "ì–‘ì‹": ["ì–‘ì‹", "ì„œì‹", "ì‹ ì²­ì„œ", "ìœ„ì„ì¥", "í™•ì¸ì„œ"]
        }
        
        keywords = []
        question_lower = question.lower()
        
        for category, words in loan_keywords.items():
            for word in words:
                if word in question_lower:
                    keywords.extend(words)
                    break
        
        # ì¤‘ë³µ ì œê±°í•˜ê³  ìƒìœ„ 3ê°œë§Œ ë°˜í™˜
        return list(set(keywords))[:3]
    
    def _call_gpt(self, prompt: str) -> str:
        """GPT API ì§ì ‘ í˜¸ì¶œ"""
        try:
            headers = {
                "Authorization": f"Bearer {self.api_key}",
                "Content-Type": "application/json"
            }
            
            data = {
                "model": "gpt-3.5-turbo",
                "messages": [{"role": "user", "content": prompt}],
                "temperature": 0.7,
                "max_tokens": 1000
            }
            
            response = requests.post(
                f"{self.base_url}/chat/completions",
                headers=headers,
                json=data,
                timeout=30
            )
            
            if response.status_code == 200:
                result = response.json()
                return result["choices"][0]["message"]["content"]
            else:
                print(f"GPT API error: {response.status_code} - {response.text}")
                return "ì£„ì†¡í•©ë‹ˆë‹¤. ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."
                
        except Exception as e:
            print(f"GPT API call error: {e}")
            return "ì£„ì†¡í•©ë‹ˆë‹¤. ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."
    
    async def generate_answer(
        self,
        question: str,
        user_id: int,
        context_docs: Optional[List[Dict]] = None
    ) -> Dict:
        """
        ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€ ìƒì„± (í•˜ì´ë¸Œë¦¬ë“œ RAG + GPT)
        1. ë¨¼ì € RAGë¡œ ì—…ë¡œë“œëœ ë¬¸ì„œì—ì„œ ë‹µë³€ ì‹œë„
        2. ë‹µë³€ í’ˆì§ˆì´ ë‚®ìœ¼ë©´ GPT APIë¡œ í´ë°±
        Args:
            question: ì‚¬ìš©ì ì§ˆë¬¸
            user_id: ì‚¬ìš©ì ID
            context_docs: ì»¨í…ìŠ¤íŠ¸ ë¬¸ì„œ (ì—†ìœ¼ë©´ ìë™ ê²€ìƒ‰)
        Returns:
            Dict: ë‹µë³€ ë° ë©”íƒ€ë°ì´í„°
        """
        start_time = datetime.utcnow()
        
        # 0ë‹¨ê³„: ì‚¬ì „ í•„í„°ë§ - ì •ë§ ëª…ë°±í•œ ë¶ˆìš©ì–´ë§Œ ì°¨ë‹¨
        if not self._is_onboarding_related(question):
            # ì‚¬ì „ í•„í„°ë§ì—ì„œ ê±¸ë¦° ì§ˆë¬¸ë„ GPTê°€ ìì—°ìŠ¤ëŸ½ê²Œ ì²˜ë¦¬í•˜ë„ë¡ í—ˆìš©
            # (ë„ˆë¬´ ë”±ë”±í•œ ì‘ë‹µ ëŒ€ì‹  GPTê°€ ìƒí™©ì— ë§ê²Œ ì‘ë‹µ)
            pass
        
        # ì€í–‰ ì˜¨ë³´ë”© ê´€ë ¨ í‚¤ì›Œë“œ í™•ì¸ (ìˆ˜ì‹ /ì—¬ì‹ /ì™¸í™˜ íŒŒíŠ¸)
        onboarding_keywords = {
            "ìˆ˜ì‹ ": ["ì˜ˆê¸ˆ", "ì ê¸ˆ", "í†µì¥", "ê³„ì¢Œ", "ì…ê¸ˆ", "ì¶œê¸ˆ", "ì´ì", "ê¸ˆë¦¬", "ìˆ˜ì‹ ", "ì˜ˆê¸ˆìë³´í˜¸", "ì‹ ê·œê°œì„¤", "í†µì¥ë°œê¸‰", "ë¹„ë°€ë²ˆí˜¸", "ì¸ì¦ì„œ"],
            "ì—¬ì‹ ": ["ëŒ€ì¶œ", "ì‹ ìš©", "ë‹´ë³´", "ì—¬ì‹ ", "dsr", "dti", "ltv", "ì—°ì²´", "ì‹ ìš©ë“±ê¸‰", "í•œë„", "ì‹ ìš©í‰ê°€", "ë‹´ë³´ì‹¬ì‚¬", "ìƒí™˜ê´€ë¦¬", "ë§ˆì´ë„ˆìŠ¤í†µì¥", "ëŒ€ì¶œì‹¬ì‚¬"],
            "ì™¸í™˜": ["ì†¡ê¸ˆ", "í™˜ì „", "ì™¸í™˜", "í•´ì™¸ì†¡ê¸ˆ", "í™˜ìœ¨", "ttë§¤ë„", "ttë§¤ì…", "ë¬´ì—­ê²°ì œ", "ì™¸í™”ì˜ˆê¸ˆ", "ì†¡ê¸ˆìˆ˜ìˆ˜ë£Œ", "ì™¸í™”ê³„ì¢Œ", "í•´ì™¸ì†¡ê¸ˆí•œë„", "í™˜ì°¨ìµ"]
        }
        
        question_lower = question.strip().lower()
        
        # ì–´ë–¤ íŒŒíŠ¸ì™€ ê´€ë ¨ëœ ì§ˆë¬¸ì¸ì§€ í™•ì¸
        detected_part = None
        for part, keywords in onboarding_keywords.items():
            if any(keyword in question_lower for keyword in keywords):
                detected_part = part
                break
        
        # ì´ì „ì—ëŠ” ì˜¨ë³´ë”© ê´€ë ¨ í‚¤ì›Œë“œê°€ ì—†ìœ¼ë©´ RAGë¥¼ ê±´ë„ˆë›°ì—ˆìœ¼ë‚˜,
        # ì´ì œëŠ” ëª¨ë“  ì§ˆë¬¸ì— ëŒ€í•´ RAGë¥¼ ìš°ì„  ì‹œë„í•œë‹¤.
        
        # 1ë‹¨ê³„: ì˜ë„ ë¶„ë¥˜ ë° ë¶€ë“œëŸ¬ìš´ ë¼ìš°íŒ…
        if context_docs is None:
            print(f"RAG ê²€ìƒ‰ ì‹œì‘: '{question}'")
            
            # ì˜ë„ ë¶„ë¥˜
            intent_result = self._classify_intent(question)
            print(f"ì˜ë„ ë¶„ë¥˜: {intent_result}")
            
            # ì¸ì‚¬ë§ ì²˜ë¦¬ (ê±°ì ˆì´ ì•„ë‹Œ í™˜ì˜ ê°€ì´ë“œ)
            if self._is_greeting(question):
                print("ì¸ì‚¬ë§ ê°ì§€ - í™˜ì˜ ê°€ì´ë“œ")
                return {
                    "answer": "ì•ˆë…•í•˜ì„¸ìš”! ğŸ» AI í•˜ë¦¬ë³´ì…ë‹ˆë‹¤!\n\ní•„ìš”í•œ ì—…ë¬´ë¥¼ ì•Œë ¤ì£¼ì„¸ìš”:\nâ€¢ 'ìœ„ì„ì¥ ì–‘ì‹ ë‹¤ìš´ë¡œë“œ'\nâ€¢ 'ì´ì˜ì‹ ì²­ì„œ ì œì¶œ ë°©ë²•'\nâ€¢ 'ì£¼íƒë‹´ë³´ëŒ€ì¶œ í•„ìˆ˜ì„œë¥˜'\nâ€¢ 'CD ìˆ˜ìµë¥  ë¬¸ì˜'\n\nì–´ë–¤ ë„ì›€ì´ í•„ìš”í•˜ì‹ ê°€ìš”? ğŸ¼",
                    "sources": [],
                    "response_time": 0.0,
                    "answer_type": "guide",
                    "mode": "GUIDE",
                    "doc_type": "general",
                    "citations": [],
                    "next_actions": ["ìœ„ì„ì¥ ì–‘ì‹", "ì´ì˜ì‹ ì²­ì„œ", "ëŒ€ì¶œ ìƒí’ˆ", "ì˜ˆê¸ˆ ìƒí’ˆ"],
                    "confidence": 1.0,
                    "notes": "ì¸ì‚¬ë§ë¡œ ë¶„ë¥˜ë¨"
                }
            
            # ì¡ë‹´ ì²˜ë¦¬ (ì¸ì‚¬ë§ì´ ì•„ë‹Œ ì§„ì§œ ì¡ë‹´)
            if intent_result["intent"] == "chitchat":
                print("ì¡ë‹´ ê°ì§€ - ì •ì¤‘ ê±°ì ˆ")
                return {
                    "answer": "ì—…ë¬´ ê´€ë ¨ ì§ˆë¬¸ ìœ„ì£¼ë¡œ ë„ì™€ë“œë ¤ìš”. ì–´ë–¤ ì—…ë¬´ê°€ ì–´ë ¤ìš°ì‹ ê°€ìš”? ğŸ»",
                    "sources": [],
                    "response_time": 0.0,
                    "answer_type": "refusal",
                    "mode": "REFUSAL",
                    "doc_type": "general",
                    "citations": [],
                    "next_actions": [],
                    "confidence": 0.0,
                    "notes": "ì¡ë‹´ìœ¼ë¡œ ë¶„ë¥˜ë¨"
                }
            
            # ì¿¼ë¦¬ í™•ì¥ (ë™ì˜ì–´, í‚¤ì›Œë“œ ì¶”ê°€)
            expanded_queries = self._expand_query(question)
            
            # ì–‘ì‹/ì„œì‹ ì „ìš© í™•ì¥ ì¶”ê°€
            if intent_result["intent"] == "onboarding" or len(question.strip()) <= 4:
                form_queries = self._expand_forms_query(question)
                expanded_queries.extend(form_queries)
                print(f"ì–‘ì‹ í™•ì¥ ì¿¼ë¦¬ ì¶”ê°€: {form_queries}")
            
            print(f"ìµœì¢… í™•ì¥ëœ ì¿¼ë¦¬: {expanded_queries}")
            
            # íƒ€ì´í‹€ ê²Œì´íŒ…ì„ í†µí•œ ê²€ìƒ‰ (ìš°ì„ )
            context_docs = self._title_gated_search(question, intent_result["intent"], k=20)
            
            # íƒ€ì´í‹€ ê²Œì´íŒ… ê²°ê³¼ê°€ ë¶€ì¡±í•˜ë©´ ì¼ë°˜ ê²€ìƒ‰ìœ¼ë¡œ ë³´ì™„
            if len(context_docs) < 5:
                print("íƒ€ì´í‹€ ê²Œì´íŒ… ê²°ê³¼ ë¶€ì¡± - ì¼ë°˜ ê²€ìƒ‰ìœ¼ë¡œ ë³´ì™„")
                additional_docs = await self.similarity_search(question, k=15)
                # ì¤‘ë³µ ì œê±°
                existing_ids = {doc['id'] for doc in context_docs}
                for doc in additional_docs:
                    if doc['id'] not in existing_ids:
                        context_docs.append(doc)
            
            print(f"ìµœì¢… RAG ê²€ìƒ‰ ê²°ê³¼: {len(context_docs)}ê°œ ë¬¸ì„œ")
            for i, doc in enumerate(context_docs):
                print(f"  {i+1}. {doc['title']} (ìœ ì‚¬ë„: {doc.get('similarity', 0):.3f})")
            
            # ì‹ ë¢°ë„ ê¸°ë°˜ í•„í„°ë§ (ë¶€ë“œëŸ¬ìš´ ë¼ìš°íŒ…)
            if context_docs:
                # ìƒìœ„ 5ê°œ ë¬¸ì„œì˜ í‰ê·  ìœ ì‚¬ë„ ê³„ì‚°
                top5_similarity = sum(doc.get('similarity', 0) for doc in context_docs[:5]) / min(5, len(context_docs))
                print(f"ìƒìœ„ 5ê°œ ë¬¸ì„œ í‰ê·  ìœ ì‚¬ë„: {top5_similarity:.3f}")
                
                # ë™ì  ì„ê³„ê°’ ì ìš©
                conf_cut, rerank_cut = self._get_dynamic_threshold(question)
                print(f"ë™ì  ì„ê³„ê°’: conf_cut={conf_cut}, rerank_cut={rerank_cut}")
                
                # ë¶€ë“œëŸ¬ìš´ ë¼ìš°íŒ… ê·œì¹™
                if top5_similarity >= conf_cut:
                    # RAG ì‚¬ìš© (ì˜¨ë³´ë”©/ì€í–‰ì¼ë°˜ êµ¬ë¶„ë§Œ ì¶œë ¥ ë¬¸êµ¬ì— ë°˜ì˜)
                    threshold = max(0.3, context_docs[0].get('similarity', 0) * 0.7)
                    context_docs = [doc for doc in context_docs if doc.get('similarity', 0) >= threshold]
                    print(f"RAG ëª¨ë“œ - í•„í„°ë§ í›„: {len(context_docs)}ê°œ ë¬¸ì„œ (ì„ê³„ê°’: {threshold:.3f})")
                else:
                    # ì‹ ë¢°ë„ ë¶€ì¡± - GPT ë°±ì—…ìœ¼ë¡œ ì ê·¹ ì „í™˜
                    print(f"ì‹ ë¢°ë„ ë¶€ì¡± ({top5_similarity:.3f} < {conf_cut}) - GPT ë°±ì—…ìœ¼ë¡œ ì „í™˜")
                    context_docs = []
        
        # ì»¨í…ìŠ¤íŠ¸ êµ¬ì„± (ì œëª©ì—ì„œ "RAG - " ì œê±°)
        context = "\n\n".join([
            f"[{doc['title'].replace('RAG - ', '')}]\n{doc['content']}"
            for doc in context_docs
        ])
        
        # ì˜¨ë³´ë”© êµìœ¡ìš© RAG ë‹µë³€ ìƒì„± (íŒŒíŠ¸ ê°ì§€ë˜ë©´ ì„¤ëª… í¬í•¨)
        part_info = {
            "ìˆ˜ì‹ ": "ê³ ê°ì´ ì€í–‰ì— ëˆì„ ë§¡ê¸°ëŠ” ì—…ë¬´ (ì˜ˆê¸ˆ, ì ê¸ˆ ë“±)",
            "ì—¬ì‹ ": "ê³ ê°ì—ê²Œ ëˆì„ ë¹Œë ¤ì£¼ëŠ” ì—…ë¬´ (ëŒ€ì¶œ, ì‹ ìš©í‰ê°€ ë“±)", 
            "ì™¸í™˜": "ì™¸êµ­ ëˆì„ ì‚¬ê³ íŒ”ê±°ë‚˜ ì†¡ê¸ˆí•˜ëŠ” ì—…ë¬´"
        }

        part_header = ""
        if detected_part:
            part_header = f"í˜„ì¬ {detected_part} íŒŒíŠ¸ êµìœ¡ ì¤‘ì´ë©°, {part_info[detected_part]}ì— ëŒ€í•´ ì„¤ëª…í•˜ê³  ìˆìŠµë‹ˆë‹¤. ğŸ¼\n\n"

        # ì˜ë„ì— ë”°ë¥¸ ëª¨ë“œ ê²°ì •
        intent_result = self._classify_intent(question)
        mode = "RAG" if context_docs else "GENERAL"
        
        # ë¬¸ì„œ íƒ€ì… ë¶„ë¥˜ ë° ìš°ì„ ìˆœìœ„
        doc_type_priority = self._get_doc_type_priority(intent_result["intent"])
        primary_doc_type = "mixed"
        if context_docs:
            # ê°€ì¥ ë§ì´ ë‚˜íƒ€ë‚˜ëŠ” ë¬¸ì„œ íƒ€ì… ê²°ì •
            doc_types = [self._classify_doc_type(doc["title"], doc.get("content", "")) for doc in context_docs]
            primary_doc_type = max(set(doc_types), key=doc_types.count) if doc_types else "general"
        
        if mode == "RAG":
            # ë¬¸ì„œ íƒ€ì…ë³„ ë‹µë³€ í…œí”Œë¦¿ ê°€ì ¸ì˜¤ê¸°
            answer_template = self._get_doc_type_template(primary_doc_type)
            
            # RAG ëª¨ë“œ í”„ë¡¬í”„íŠ¸
            rag_prompt = (
                "ë‹¹ì‹ ì€ 'ì€í–‰ ì˜¨ë³´ë”©+ì¼ë°˜ ê¸ˆìœµ' RAG ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤. ğŸ»\n"
                f"{part_header}"
                "ë‹¤ìŒ ê²€ìƒ‰ ì»¨í…ìŠ¤íŠ¸ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ë‹µë³€í•˜ì„¸ìš”:\n\n"
                f"{context}\n\n"
                f"ì§ˆë¬¸: {question}\n"
                f"ì˜ë„: {intent_result['intent']} / ìš°ì„  ë¬¸ì„œíƒ€ì…: {doc_type_priority}\n"
                f"ì£¼ìš” ë¬¸ì„œíƒ€ì…: {primary_doc_type}\n\n"
                "ë‹µë³€ ìš”êµ¬ì‚¬í•­:\n"
                "- ì‹ ì…ì‚¬ì›ì´ ì´í•´í•˜ê¸° ì‰½ê²Œ ì¹œì ˆí•˜ê³  ìƒì„¸í•˜ê²Œ ë‹µë³€í•˜ì„¸ìš”\n"
                "- ìì—°ìŠ¤ëŸ¬ìš´ ë¬¸ë‹¨ìœ¼ë¡œ êµ¬ì„±í•˜ì—¬ ê¸¸ê²Œ ì„¤ëª…í•˜ì„¸ìš” (ë¶ˆë¦¿ í¬ì¸íŠ¸ë‚˜ ë‹¨ê³„ë³„ ë‚˜ì—´ ê¸ˆì§€)\n"
                "- í•µì‹¬ ì •ë³´ë¥¼ ë¨¼ì € ì œì‹œí•˜ê³ , ì„¸ë¶€ì‚¬í•­ì„ í¬í•¨í•œ ì™„ì „í•œ ì„¤ëª…ì„ ì œê³µí•˜ì„¸ìš”\n"
                "- ë¬¸ë‹¨ê³¼ ë¬¸ë‹¨ ì‚¬ì´ëŠ” ìì—°ìŠ¤ëŸ½ê²Œ ì—°ê²°í•˜ì—¬ íë¦„ ìˆê²Œ ì‘ì„±í•˜ì„¸ìš”\n"
                "- í•œêµ­ì–´ë¡œ ì¹œê·¼í•˜ê²Œ ë‹µë³€í•˜ì„¸ìš” (ğŸ» ì´ëª¨í‹°ì½˜ì€ ë‹µë³€ ì‹œì‘ì—ë§Œ ì‚¬ìš©)\n"
                "- AI í•˜ë¦¬ë³´ë¡œì„œ ì‹ ì…ì‚¬ì›ì„ ë„ì™€ì£¼ëŠ” ë§ˆìŒìœ¼ë¡œ ë‹µë³€í•˜ì„¸ìš”\n"
                "- ë¬¸ì„œ ë‚´ìš©ì´ ìˆìœ¼ë©´ ë°˜ë“œì‹œ ë¬¸ì„œ ë‚´ìš©ì„ ìš°ì„  í™œìš©í•˜ê³ , ë¬¸ì„œ ë‚´ìš©ì— ì—†ëŠ” ë¶€ë¶„ë§Œ ì¼ë°˜ì ì¸ ì€í–‰ ì—…ë¬´ ì§€ì‹ìœ¼ë¡œ ë³´ì™„í•˜ì„¸ìš”\n"
                "- ì§ˆë¬¸ê³¼ ê´€ë ¨ ì—†ëŠ” ë‚´ìš©ì€ ë‹µë³€ì— í¬í•¨í•˜ì§€ ë§ˆì„¸ìš”\n"
                "- ì§ˆë¬¸ì˜ í•µì‹¬ í‚¤ì›Œë“œì™€ ì§ì ‘ ê´€ë ¨ëœ ë‚´ìš©ë§Œ ë‹µë³€í•˜ì„¸ìš”\n"
                "- ë¬¸ì¥ì„ ì¶©ë¶„íˆ ê¸¸ê²Œ ì‘ì„±í•˜ì—¬ ìƒì„¸í•œ ì„¤ëª…ì„ ì œê³µí•˜ì„¸ìš”\n"
                "- ê° ë¬¸ë‹¨ì€ 5-7ë¬¸ì¥ìœ¼ë¡œ êµ¬ì„±í•˜ì—¬ ì¶©ë¶„í•œ ì •ë³´ë¥¼ ì œê³µí•˜ì„¸ìš”\n"
                "- ìƒí’ˆ ì¶”ì²œ ê´€ë ¨ ì§ˆë¬¸ì—ëŠ” êµ¬ì²´ì ì¸ ìƒí’ˆëª…ê³¼ íŠ¹ì§•ì„ í¬í•¨í•˜ì—¬ ë‹µë³€í•˜ì„¸ìš”\n"
                "- ì—°ë ¹ëŒ€ë³„ ê³ ê° íŠ¹ì„±ì„ ê³ ë ¤í•œ ë§ì¶¤í˜• ìƒí’ˆ ì¶”ì²œì„ ì œê³µí•˜ì„¸ìš”\n"
                "- ëŒ€ì¶œ ìƒí’ˆì˜ ê²½ìš° ê¸ˆë¦¬, í•œë„, ìƒí™˜ì¡°ê±´ ë“± êµ¬ì²´ì ì¸ ì •ë³´ë¥¼ í¬í•¨í•˜ì„¸ìš”\n"
                "- ëŒ€ì¶œ ìƒë‹´ ì§ˆë¬¸ì—ëŠ” ì‹¤ì œ ëŒ€ì¶œ ìƒí’ˆì„ ì¶”ì²œí•˜ê³ , ì•Œë¦¼ ì„œë¹„ìŠ¤ë‚˜ ê¸°íƒ€ ì„œë¹„ìŠ¤ëŠ” ì¶”ì²œí•˜ì§€ ë§ˆì„¸ìš”\n"
                "- ë‹µë³€ì„ ìì—°ìŠ¤ëŸ¬ìš´ ë¬¸ë‹¨ìœ¼ë¡œ êµ¬ì„±í•˜ì—¬ ê¸¸ê³  ìƒì„¸í•˜ê²Œ ì‘ì„±í•˜ì„¸ìš”\n"
                "- ëŒ€ì¶œ ìƒí’ˆ ì¶”ì²œ ì‹œì—ëŠ” êµ¬ì²´ì ì¸ ìƒí’ˆëª…(ê°€ê³„ëŒ€ì¶œ, ì£¼íƒë‹´ë³´ëŒ€ì¶œ, ì „ì›”ì„¸ë³´ì¦ê¸ˆëŒ€ì¶œ ë“±)ì„ ëª…ì‹œí•˜ì„¸ìš”\n"
                "- 70ëŒ€ ê³ ê°ì—ê²ŒëŠ” ì—°ë ¹ëŒ€ì— ë§ëŠ” ëŒ€ì¶œ ìƒí’ˆì„ êµ¬ì²´ì ìœ¼ë¡œ ì¶”ì²œí•˜ì„¸ìš”\n"
                "- ëŒ€ì¶œ ìƒë‹´ ì§ˆë¬¸ì—ëŠ” ë°˜ë“œì‹œ êµ¬ì²´ì ì¸ ëŒ€ì¶œ ìƒí’ˆëª…ì„ ì¶”ì²œí•˜ê³ , ë‹¨ê³„ë³„ ì„¤ëª…ì´ë‚˜ ì¼ë°˜ì ì¸ ì¡°ì–¸ì€ í”¼í•˜ì„¸ìš”\n"
                "- ê³ ê°ì—ê²Œ ì§ì ‘ì ì¸ ëŒ€ì¶œ ìƒí’ˆ ì¶”ì²œì„ ì œê³µí•˜ì„¸ìš”\n\n"
                f"ë‹µë³€ í˜•ì‹ (ë¬¸ì„œíƒ€ì…: {primary_doc_type}):\n{answer_template}\n"
                "ë‹¤ìŒ JSON í˜•ì‹ìœ¼ë¡œë§Œ ì¶œë ¥í•˜ë¼:\n"
                "{\n"
                '  "mode": "RAG",\n'
                f'  "doc_type": "{primary_doc_type}",\n'
                '  "answer": "í•µì‹¬ ë‹µë³€ ë‚´ìš©",\n'
                '  "citations": [\n'
                '    {"doc_title": "ë¬¸ì„œì œëª©", "section": "ì„¹ì…˜", "page": 0, "clause_id": ""}\n'
                '  ],\n'
                '  "next_actions": ["ì§€ì  ì˜ˆì•½", "í•„ìš”ì„œë¥˜ í™•ì¸", "ë‹¤ìš´ë¡œë“œ ë§í¬ ì—´ê¸°"],\n'
                '  "confidence": 0.8,\n'
                '  "notes": "ì„ íƒ: ê°€ì •/ì œì•½/ì¶”ê°€í™•ì¸ ì•ˆë‚´"\n'
                "}\n\n"
                "ë‹µë³€:"
            )
        else:
            # GENERAL ëª¨ë“œ í”„ë¡¬í”„íŠ¸
            rag_prompt = (
                "ë‹¹ì‹ ì€ ì€í–‰ ì¼ë°˜ ìƒë‹´ ë„ìš°ë¯¸ì…ë‹ˆë‹¤. ğŸ»\n"
                f"ì§ˆë¬¸: {question}\n\n"
                "ë‹µë³€ ìš”êµ¬ì‚¬í•­:\n"
                "- ì‹ ì…ì‚¬ì›ì´ ì´í•´í•˜ê¸° ì‰½ê²Œ ì¹œì ˆí•˜ê³  ìƒì„¸í•˜ê²Œ ë‹µë³€í•˜ì„¸ìš”\n"
                "- ìì—°ìŠ¤ëŸ¬ìš´ ë¬¸ë‹¨ìœ¼ë¡œ êµ¬ì„±í•˜ì—¬ ê¸¸ê²Œ ì„¤ëª…í•˜ì„¸ìš” (ë¶ˆë¦¿ í¬ì¸íŠ¸ë‚˜ ë‹¨ê³„ë³„ ë‚˜ì—´ ê¸ˆì§€)\n"
                "- í•µì‹¬ ì •ë³´ë¥¼ ë¨¼ì € ì œì‹œí•˜ê³ , ì„¸ë¶€ì‚¬í•­ì„ í¬í•¨í•œ ì™„ì „í•œ ì„¤ëª…ì„ ì œê³µí•˜ì„¸ìš”\n"
                "- ë¬¸ë‹¨ê³¼ ë¬¸ë‹¨ ì‚¬ì´ëŠ” ìì—°ìŠ¤ëŸ½ê²Œ ì—°ê²°í•˜ì—¬ íë¦„ ìˆê²Œ ì‘ì„±í•˜ì„¸ìš”\n"
                "- í•œêµ­ì–´ë¡œ ì¹œê·¼í•˜ê²Œ ë‹µë³€í•˜ì„¸ìš” (ğŸ» ì´ëª¨í‹°ì½˜ì€ ë‹µë³€ ì‹œì‘ì—ë§Œ ì‚¬ìš©)\n"
                "- AI í•˜ë¦¬ë³´ë¡œì„œ ì‹ ì…ì‚¬ì›ì„ ë„ì™€ì£¼ëŠ” ë§ˆìŒìœ¼ë¡œ ë‹µë³€í•˜ì„¸ìš”\n"
                "- ì€í–‰ ì—…ë¬´ì— ê´€ë ¨ëœ ì‹¤ìš©ì ì¸ ì¡°ì–¸ì„ í¬í•¨í•˜ì„¸ìš”\n"
                "- ë¬¸ì¥ì„ ì¶©ë¶„íˆ ê¸¸ê²Œ ì‘ì„±í•˜ì—¬ ìƒì„¸í•œ ì„¤ëª…ì„ ì œê³µí•˜ì„¸ìš”\n"
                "- ê° ë¬¸ë‹¨ì€ 5-7ë¬¸ì¥ìœ¼ë¡œ êµ¬ì„±í•˜ì—¬ ì¶©ë¶„í•œ ì •ë³´ë¥¼ ì œê³µí•˜ì„¸ìš”\n"
                "- ìƒí’ˆ ì¶”ì²œ ê´€ë ¨ ì§ˆë¬¸ì—ëŠ” êµ¬ì²´ì ì¸ ìƒí’ˆëª…ê³¼ íŠ¹ì§•ì„ í¬í•¨í•˜ì—¬ ë‹µë³€í•˜ì„¸ìš”\n"
                "- ì—°ë ¹ëŒ€ë³„ ê³ ê° íŠ¹ì„±ì„ ê³ ë ¤í•œ ë§ì¶¤í˜• ìƒí’ˆ ì¶”ì²œì„ ì œê³µí•˜ì„¸ìš”\n"
                "- ëŒ€ì¶œ ìƒí’ˆì˜ ê²½ìš° ê¸ˆë¦¬, í•œë„, ìƒí™˜ì¡°ê±´ ë“± êµ¬ì²´ì ì¸ ì •ë³´ë¥¼ í¬í•¨í•˜ì„¸ìš”\n"
                "- ëŒ€ì¶œ ìƒë‹´ ì§ˆë¬¸ì—ëŠ” ì‹¤ì œ ëŒ€ì¶œ ìƒí’ˆì„ ì¶”ì²œí•˜ê³ , ì•Œë¦¼ ì„œë¹„ìŠ¤ë‚˜ ê¸°íƒ€ ì„œë¹„ìŠ¤ëŠ” ì¶”ì²œí•˜ì§€ ë§ˆì„¸ìš”\n"
                "- ë‹µë³€ì„ ìì—°ìŠ¤ëŸ¬ìš´ ë¬¸ë‹¨ìœ¼ë¡œ êµ¬ì„±í•˜ì—¬ ê¸¸ê³  ìƒì„¸í•˜ê²Œ ì‘ì„±í•˜ì„¸ìš”\n"
                "- ëŒ€ì¶œ ìƒí’ˆ ì¶”ì²œ ì‹œì—ëŠ” êµ¬ì²´ì ì¸ ìƒí’ˆëª…(ê°€ê³„ëŒ€ì¶œ, ì£¼íƒë‹´ë³´ëŒ€ì¶œ, ì „ì›”ì„¸ë³´ì¦ê¸ˆëŒ€ì¶œ ë“±)ì„ ëª…ì‹œí•˜ì„¸ìš”\n"
                "- 70ëŒ€ ê³ ê°ì—ê²ŒëŠ” ì—°ë ¹ëŒ€ì— ë§ëŠ” ëŒ€ì¶œ ìƒí’ˆì„ êµ¬ì²´ì ìœ¼ë¡œ ì¶”ì²œí•˜ì„¸ìš”\n"
                "- ëŒ€ì¶œ ìƒë‹´ ì§ˆë¬¸ì—ëŠ” ë°˜ë“œì‹œ êµ¬ì²´ì ì¸ ëŒ€ì¶œ ìƒí’ˆëª…ì„ ì¶”ì²œí•˜ê³ , ë‹¨ê³„ë³„ ì„¤ëª…ì´ë‚˜ ì¼ë°˜ì ì¸ ì¡°ì–¸ì€ í”¼í•˜ì„¸ìš”\n"
                "- ê³ ê°ì—ê²Œ ì§ì ‘ì ì¸ ëŒ€ì¶œ ìƒí’ˆ ì¶”ì²œì„ ì œê³µí•˜ì„¸ìš”\n"
                "- ì •ì±…/ë‚´ê·œì²˜ëŸ¼ ì§€ì Â·ì‹œê¸°ë³„ë¡œ ë‹¬ë¼ì§ˆ ìˆ˜ ìˆëŠ” ë‚´ìš©ì€ 'ì¼ë°˜ì  ê¸°ì¤€'ìœ¼ë¡œë§Œ ì„¤ëª…í•˜ê³ , í™•ì¸ ì ˆì°¨/ì¤€ë¹„ ì„œë¥˜/ë¬¸ì˜ ê²½ë¡œë¥¼ í•¨ê»˜ ì•ˆë‚´í•œë‹¤\n"
                "- ì˜¨ë³´ë”© ë¬¸ë§¥ê³¼ ì—°ê²°ë  ìˆ˜ ìˆìœ¼ë©´ ì¶”ê°€ë¡œ 'ê´€ë ¨ ì˜¨ë³´ë”© í•­ëª©'ì„ ì œì‹œí•œë‹¤\n"
                "- ê°€ëŠ¥í•œ í•œ ë‹¤ìŒ í–‰ë™(ìƒë‹´ ì˜ˆì•½/í•„ìš” ì„œë¥˜/ë©”ë‰´ ê²½ë¡œ)ì„ ì œì•ˆí•œë‹¤\n\n"
                "ë‹¤ìŒ JSON í˜•ì‹ìœ¼ë¡œë§Œ ì¶œë ¥í•˜ë¼:\n"
                "{\n"
                '  "mode": "GENERAL",\n'
                '  "answer": "ì¼ë°˜ ì •ë³´ ë‹µë³€",\n'
                '  "citations": [],\n'
                '  "confidence": 0.6,\n'
                '  "notes": "ì¼ë°˜ ì •ë³´ - ì§€ì ë³„ ìƒì´ ê°€ëŠ¥"\n'
                "}\n\n"
                "ë‹µë³€:"
            )
        
        # RAG ë‹µë³€ ìƒì„±
        rag_answer = self._call_gpt(rag_prompt)
        
        # JSON ì‘ë‹µ íŒŒì‹± ì‹œë„
        try:
            import json
            # JSON ì‘ë‹µì—ì„œ ë‹µë³€ ì¶”ì¶œ
            if rag_answer.strip().startswith('{'):
                json_response = json.loads(rag_answer)
                final_answer = json_response.get("answer", rag_answer)
                answer_mode = json_response.get("mode", mode)
                answer_doc_type = json_response.get("doc_type", primary_doc_type)
                citations = json_response.get("citations", [])
                next_actions = json_response.get("next_actions", [])
                confidence = json_response.get("confidence", 0.8)
                notes = json_response.get("notes", "")
            else:
                # JSONì´ ì•„ë‹Œ ê²½ìš° ê¸°ì¡´ ë¡œì§ ì‚¬ìš©
                final_answer = rag_answer
                answer_mode = mode
                answer_doc_type = primary_doc_type
                citations = []
                next_actions = []
                confidence = 0.8
                notes = ""
        except:
            # JSON íŒŒì‹± ì‹¤íŒ¨ ì‹œ ê¸°ì¡´ ë¡œì§ ì‚¬ìš©
            final_answer = rag_answer
            answer_mode = mode
            answer_doc_type = primary_doc_type
            citations = []
            next_actions = []
            confidence = 0.8
            notes = ""
        
        # ë‹µë³€ í’ˆì§ˆ í‰ê°€ (ì¤‘ìš” ë‹¨ì–´ í•„í„°ë§ í¬í•¨)
        is_rag_adequate = self._evaluate_answer_quality(final_answer, context_docs, question)
        
        if is_rag_adequate and context_docs:
            # RAG ë‹µë³€ì´ ì ì ˆí•˜ë©´ ê·¸ëŒ€ë¡œ ì‚¬ìš©
            # ê³ í’ˆì§ˆ ë¬¸ì„œë§Œ ì°¸ê³ ìë£Œë¡œ ì‚¬ìš© (ì„ê³„ê°’ ë‚®ì¶¤)
            high_quality_docs = [doc for doc in context_docs if doc.get('similarity', 0) >= 0.50]
            
            answer_type = "rag"
            context_docs = high_quality_docs  # ê³ í’ˆì§ˆ ë¬¸ì„œë§Œ ì „ë‹¬
        else:
            # RAG ë‹µë³€ì´ ë¶€ì ì ˆí•˜ê±°ë‚˜ ë¬¸ì„œê°€ ì—†ìœ¼ë©´ GPT ë°±ì—… (ì ê·¹ í™œìš©)
            print("RAG ì‹¤íŒ¨ - GPT ë°±ì—…ìœ¼ë¡œ ì „í™˜ (ì ê·¹ í™œìš©)")
            gpt_answer = await self._generate_gpt_fallback(question, context_docs)
            final_answer = gpt_answer
            answer_type = "gpt_backup"
            answer_mode = "GENERAL"
            
            # GPT ë°±ì—…ìš© ê´€ë ¨ ë¬¸ì„œ ì¶”ì²œ
            related_docs = self._get_related_documents_for_gpt(question)
            if related_docs:
                # GPT ë°±ì—… ì‹œì—ë„ sources ì •ë³´ í¬í•¨
                context_docs = [{"title": doc, "content": "", "similarity": 0.5} for doc in related_docs[:3]]
                citations = [{"doc_title": doc, "section": "", "page": 0, "clause_id": ""} for doc in related_docs[:3]]
            else:
                context_docs = []
                citations = []
            confidence = 0.6
            notes = "GPT ë°±ì—… ë‹µë³€"
        
        # ì‘ë‹µ ì‹œê°„ ê³„ì‚°
        response_time = (datetime.utcnow() - start_time).total_seconds()
        
        # ëŒ€í™” ê¸°ë¡ ì €ì¥ (ì•ˆì „í•œ íŠ¸ëœì­ì…˜)
        try:
            chat_history = ChatHistory(
                user_id=user_id,
                user_message=question,
                bot_response=final_answer,
                source_documents=json.dumps([
                    {"title": doc["title"], "category": doc["category"], "type": answer_type}
                    for doc in context_docs
                ], ensure_ascii=False),
                response_time=response_time
            )
            self.session.add(chat_history)
            self.session.commit()
        except Exception as e:
            print(f"Chat history save error: {e}")
            self.session.rollback()
        
        return {
            "answer": final_answer,
            "sources": context_docs,
            "response_time": response_time,
            "answer_type": answer_type,
            "mode": answer_mode,
            "doc_type": answer_doc_type,
            "citations": citations,
            "next_actions": next_actions,
            "confidence": confidence,
            "notes": notes
        }
    
    def _evaluate_answer_quality(self, answer: str, context_docs: List[Dict], question: str = "") -> bool:
        """
        RAG ë‹µë³€ì˜ í’ˆì§ˆ í‰ê°€ (ì¤‘ìš” ë‹¨ì–´ í•„í„°ë§ í¬í•¨)
        Args:
            answer: ìƒì„±ëœ ë‹µë³€
            context_docs: ì°¸ê³  ë¬¸ì„œë“¤
            question: ì›ë³¸ ì§ˆë¬¸
        Returns:
            bool: ë‹µë³€ì´ ì ì ˆí•œì§€ ì—¬ë¶€
        """
        if not answer or len(answer.strip()) < 20:
            print("Answer too short")
            return False

        # ì»¨í…ìŠ¤íŠ¸ ë¬¸ì„œê°€ ì—†ìœ¼ë©´ RAG ë‹µë³€ìœ¼ë¡œ ë¶€ì ì ˆ (í•˜ì§€ë§Œ GPT ë°±ì—… í—ˆìš©)
        if not context_docs or len(context_docs) == 0:
            print("No context documents found - GPT ë°±ì—… í—ˆìš©")
            return True  # GPT ë°±ì—…ì„ í—ˆìš©í•˜ë„ë¡ Trueë¡œ ë³€ê²½

        # ì¤‘ìš” ë‹¨ì–´ í•„í„°ë§ - ì§ˆë¬¸ì˜ í•µì‹¬ í‚¤ì›Œë“œê°€ ë¬¸ì„œì— ìˆëŠ”ì§€ í™•ì¸
        if question:
            question_lower = question.lower()
            # ì§ˆë¬¸ì—ì„œ ì¤‘ìš”í•œ í‚¤ì›Œë“œ ì¶”ì¶œ (2ê¸€ì ì´ìƒ)
            important_keywords = [word for word in question_lower.split() if len(word) >= 2]
            
            # ë¬¸ì„œ ë‚´ìš©ì— ì¤‘ìš”í•œ í‚¤ì›Œë“œê°€ í¬í•¨ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸
            has_important_keywords = False
            for doc in context_docs:
                doc_content_lower = doc.get('content', '').lower()
                if any(keyword in doc_content_lower for keyword in important_keywords):
                    has_important_keywords = True
                    break
            
            if not has_important_keywords:
                print("No important keywords found in RAG documents - GPT ë°±ì—… í—ˆìš©")
                return True  # GPT ë°±ì—…ì„ í—ˆìš©í•˜ë„ë¡ Trueë¡œ ë³€ê²½
        
        # ë¶€ì •ì ì¸ ì§€í‘œë“¤
        negative_indicators = [
            "ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤",
            "ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤",
            "ì œê³µëœ ìë£Œì—ì„œ",
            "ëª¨ë¥´ê² ìŠµë‹ˆë‹¤",
            "ì•Œ ìˆ˜ ì—†ìŠµë‹ˆë‹¤",
            "cannot find",
            "not found",
            "no information",
            "unable to find"
        ]
        
        answer_lower = answer.lower()
        for indicator in negative_indicators:
            if indicator in answer_lower:
                print(f"Negative indicator found: {indicator} - GPT ë°±ì—…ì—ì„œëŠ” í—ˆìš©")
                return True  # GPT ë°±ì—…ì—ì„œëŠ” í—ˆìš©
        
        # ìœ ì‚¬ë„ ì„ê³„ê°’ì„ ëŒ€í­ ì™„í™”í•˜ì—¬ ë” ë§ì€ ë¬¸ì„œë¥¼ í—ˆìš©
        if context_docs and all(doc.get('similarity', 0) < 0.20 for doc in context_docs):
            print("All documents have low similarity - GPT ë°±ì—… í—ˆìš©")
            return True  # GPT ë°±ì—… í—ˆìš©

        # ê´€ë ¨ì„± ë†’ì€ ë¬¸ì„œê°€ ìˆëŠ”ì§€ í™•ì¸ (ì„ê³„ê°’ ëŒ€í­ ì™„í™”)
        high_quality_docs = [doc for doc in context_docs if doc.get('similarity', 0) >= 0.20]
        if not high_quality_docs:
            print("No high-quality relevant documents found - GPT ë°±ì—… í—ˆìš©")
            return True  # GPT ë°±ì—… í—ˆìš©

        print(f"Answer quality check passed. High-quality docs: {len(high_quality_docs)}")
        return True
    
    async def _generate_gpt_fallback(self, question: str, context_docs: List[Dict]) -> str:
        """
        ì˜¨ë³´ë”© êµìœ¡ìš© GPT í´ë°± ë‹µë³€ ìƒì„±
        """
        # ê°„ë‹¨í•œ ì¸ì‚¬ ì²˜ë¦¬
        if any(greeting in question.lower() for greeting in ["ì•ˆë…•", "ì•ˆë…•í•˜ì„¸ìš”", "í•˜ì´", "hi", "hello"]):
            return "ì•ˆë…•í•˜ì„¸ìš”! ğŸ» ì €ëŠ” AI í•˜ë¦¬ë³´ì˜ˆìš”! ğŸ¼ í•˜ê²½ì€í–‰ ì‹ ì…ì‚¬ì› ì˜¨ë³´ë”©ì„ ë„ì™€ë“œë¦½ë‹ˆë‹¤. ìˆ˜ì‹ (ì˜ˆê¸ˆ), ì—¬ì‹ (ëŒ€ì¶œ), ì™¸í™˜(í™˜ì „Â·ì†¡ê¸ˆ) íŒŒíŠ¸ ì¤‘ ê¶ê¸ˆí•œ ì—…ë¬´ê°€ ìˆìœ¼ì‹œë©´ ì–¸ì œë“  ë¬¼ì–´ë³´ì„¸ìš”! ğŸ»â€â„ï¸"
        
        # GPT ë°±ì—…ìš© ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ (RAG ì‹¤íŒ¨ ì‹œ) - ì ê·¹ì  í™œìš©
        system_prompt = """ë‹¹ì‹ ì€ 'ì€í–‰ ì‹ ì…ì‚¬ì› ì˜¨ë³´ë”© ë„ìš°ë¯¸'ì…ë‹ˆë‹¤. ğŸ»

RAG ì‹œìŠ¤í…œì—ì„œ ê´€ë ¨ ìë£Œë¥¼ ì°¾ì§€ ëª»í–ˆì„ ë•Œë„ ê°€ëŠ¥í•œ í•œ ë„ì›€ì´ ë˜ëŠ” ë‹µë³€ì„ ì œê³µí•˜ì„¸ìš”.

ë‹¤ìŒ ì‚¬í•­ì„ ë°˜ë“œì‹œ ëª…ì‹œí•˜ì„¸ìš”:
1. ì •ì±…ì€ ì§€ì /ì‹œê¸°ë³„ë¡œ ìƒì´í•  ìˆ˜ ìˆìŒì„ ì•Œë¦¬ê¸°
2. ì¶”ê°€ í™•ì¸ ì ˆì°¨/ë‹´ë‹¹ ë¶€ì„œë¥¼ ì•ˆë‚´í•˜ê¸°
3. "ì˜¨ë³´ë”© ê´€ë ¨ ì§ˆë¬¸ë§Œ ë‹µë³€í•©ë‹ˆë‹¤" ê°™ì€ ê±°ì ˆ ë©”ì‹œì§€ëŠ” ì ˆëŒ€ ì‚¬ìš©í•˜ì§€ ë§ˆì„¸ìš”
4. ë‹µë³€ ë§ˆì§€ë§‰ì— ê´€ë ¨ ìë£Œì‹¤ ë¬¸ì„œë¥¼ ì¶”ì²œí•˜ì„¸ìš”

ì˜¨ë³´ë”© ê´€ë ¨ ì§ˆë¬¸ì— ëŒ€í•´ì„œëŠ” ë‹¤ìŒ í˜•ì‹ìœ¼ë¡œ ë‹µë³€í•˜ì„¸ìš”:
- â‘  í•µì‹¬ ê°œë… ìš”ì•½ (í•œ ë¬¸ë‹¨) ğŸ»
- â‘¡ ì‹¤ì œ í˜„ì¥ ì˜ˆì‹œ (êµ¬ì²´ì ì¸ ìƒí™©ì´ë‚˜ ìˆ«ì í¬í•¨) ğŸ¼
- â‘¢ ì‹¤ë¬´ ìœ ì˜ì‚¬í•­ (ì£¼ì˜í•  ì ì´ë‚˜ íŒ) ğŸ»â€â„ï¸
- â‘£ ê´€ë ¨ ìë£Œì‹¤ ë¬¸ì„œ ì¶”ì²œ (ì˜ˆ: "ìë£Œì‹¤ì—ì„œ 'ìœ„ì„ì¥ ì–‘ì‹'ì„ ê²€ìƒ‰í•´ë³´ì„¸ìš”")

ë‹µë³€í•  ë•ŒëŠ” í•­ìƒ ê³° ì´ëª¨í‹°ì½˜(ğŸ», ğŸ¼, ğŸ»â€â„ï¸)ì„ ì ì ˆíˆ ì‚¬ìš©í•˜ì—¬ ì¹œê·¼í•˜ê³  ê·€ì—¬ìš´ ëŠë‚Œì„ ì£¼ì„¸ìš”.
ë‹µë³€ì€ ì¹œê·¼í•˜ê³  ë„ì›€ì´ ë˜ëŠ” í†¤ìœ¼ë¡œ ì‘ì„±í•˜ì„¸ìš”."""
        
        gpt_prompt = f"""{system_prompt}

ì§ˆë¬¸: {question}

ë‹µë³€:"""
        
        # GPTë¡œ ë‹µë³€ ìƒì„±
        gpt_answer = self._call_gpt(gpt_prompt)
        
        return gpt_answer
    
    def _get_related_documents_for_gpt(self, question: str) -> List[str]:
        """
        GPT ë°±ì—…ìš© ê´€ë ¨ ë¬¸ì„œ ì¶”ì²œ
        """
        query_lower = question.lower()
        related_docs = []
        
        # ì§ˆë¬¸ í‚¤ì›Œë“œ ê¸°ë°˜ ë¬¸ì„œ ì¶”ì²œ
        doc_recommendations = {
            "ìœ„ì„ì¥": ["ìœ„ì„ì¥(ê°œì¸ì—¬ì‹ )", "ìœ„ì„ì¥(ê¸ˆê²°ì›+ëŒ€ì¶œì´ë™ì‹œìŠ¤í…œ)", "ìœ„ì„ì¥+(ì „ì›”ì„¸ëŒ€ì¶œ+ì‹¬ì‚¬ìš©)"],
            "ì´ì˜ì‹ ì²­": ["ì´ì˜ì‹ ì²­ì„œ", "ì´ì˜ì œê¸°ì‹ ì²­ì„œ", "í”¼í•´êµ¬ì œì‹ ì²­ì„œ"],
            "ì‹ ì²­ì„œ": ["ë¯¼ì›ì ‘ìˆ˜ì–‘ì‹", "ì œì¦ëª…ì˜ë¢°ì„œ", "ìë£Œì—´ëŒìš”êµ¬ì„œ"],
            "ì•½ê´€": ["ì˜ˆê¸ˆê±°ë˜ê¸°ë³¸ì•½ê´€", "ì „ì„¸ì§€í‚´ë³´ì¦ì•½ê´€", "ì „ìê¸ˆìœµê±°ë˜ê¸°ë³¸ì•½ê´€"],
            "ëŒ€ì¶œ": ["ê°€ê³„ëŒ€ì¶œìƒí’ˆì„¤ëª…ì„œ", "ê¸°ì—…ëŒ€ì¶œìƒí’ˆì„¤ëª…ì„œ", "ì „ì›”ì„¸ë³´ì¦ê¸ˆëŒ€ì¶œ+ìƒí’ˆì„¤ëª…ì„œ"],
            "ì˜ˆê¸ˆ": ["ê±°ì¹˜ì‹ì˜ˆê¸ˆì•½ê´€", "ì ë¦½ì‹ì˜ˆê¸ˆì•½ê´€", "ì…ì¶œê¸ˆì´ììœ ë¡œìš´ì˜ˆê¸ˆ_ì•½ê´€"],
            "ë³´ì¦": ["ì‹ ìš©ë³´ì¦ì„œ", "ì‹ ìš©ë³´ì¦ì•½ì •ì„œ", "ë³´ì¦ì±„ë¬´ì´í–‰ì²­êµ¬ì„œ"],
            "ë§Œê¸°": ["ëŒ€ì¶œë§Œê¸°ì•ˆë‚´", "ëŒ€ì¶œë§Œê¸°ê²½ê³¼ì•ˆë‚´"],
            "ê³„ì¢Œ": ["ì±„ê¶Œìê³„ì¢Œì‹ ê³ ì„œ", "ì¦ê¶Œê³„ì¢Œê°œì„¤ì„œë¹„ìŠ¤+ì„¤ëª…ì„œ"],
            "ìƒí™©í‘œ": ["ìˆ˜ì‹ ê±°ë˜ìƒí™©í‘œ", "ì—¬ì‹ ê±°ë˜ìƒí™©í‘œ"],
            "í™•ì¸ì„œ": ["ì”ì¡´ì±„ë¬´í™•ì¸ì„œ", "ì´ìê³„ì‚°ë‚´ì—­ì„œ"],
            "ì•ˆë‚´": ["ê³ ê°ê¶Œë¦¬ì•ˆë‚´ë¬¸", "ëŒ€ì¶œë§Œê¸°ì•ˆë‚´"],
            "íŠ¹ì•½": ["ë¹„ê³¼ì„¸ì¢…í•©ì €ì¶•íŠ¹ì•½"],
            "í†µì§€ì„œ": ["ì‹ ìš©ë³´ì¦ë¶€ì‹¤í†µì§€ì„œ"],
            "ì„œë¹„ìŠ¤": ["ê³„ì¢Œí†µí•©ê´€ë¦¬ì„œë¹„ìŠ¤+ì´ìš©ì•½ê´€", "ì¦ê¶Œê³„ì¢Œê°œì„¤ì„œë¹„ìŠ¤_ì´ìš©ì•½ê´€"]
        }
        
        # í‚¤ì›Œë“œ ë§¤ì¹­ìœ¼ë¡œ ê´€ë ¨ ë¬¸ì„œ ì¶”ì²œ
        for keyword, docs in doc_recommendations.items():
            if keyword in query_lower:
                related_docs.extend(docs)
        
        # ì¤‘ë³µ ì œê±°
        return list(set(related_docs))
    
    def _classify_intent(self, query: str) -> dict:
        """
        ì˜ë„ ë¶„ë¥˜ - ì˜¨ë³´ë”©/ì€í–‰ì¼ë°˜/ì¡ë‹´ êµ¬ë¶„
        """
        query_lower = query.lower()
        
        # ì˜¨ë³´ë”© ê´€ë ¨ í‚¤ì›Œë“œ (ë†’ì€ ê°€ì¤‘ì¹˜) - í™•ì¥
        onboarding_keywords = {
            "ì˜¨ë³´ë”©": 1.0, "ì‹ ì…ì‚¬ì›": 1.0, "êµìœ¡": 0.9, "í›ˆë ¨": 0.9,
            "ë‚´ê·œ": 0.9, "ê·œì •": 0.8, "ë§¤ë‰´ì–¼": 0.8, "ì ˆì°¨": 0.8,
            "ì‹œìŠ¤í…œ": 0.7, "ì—…ë¬´": 0.7, "í”„ë¡œì„¸ìŠ¤": 0.7, "ê°€ì´ë“œ": 0.7,
            "ì‹¬ì‚¬": 0.6, "ì„œë¥˜": 0.6, "ë³´ë¥˜": 0.6, "ìŠ¹ì¸": 0.6,
            "ìœ„ì„ì¥": 0.8, "ì‹ ì²­ì„œ": 0.8, "ì´ì˜ì œê¸°": 0.8, "ì‹ ì²­": 0.7,
            "ì–‘ì‹": 0.7, "ì„œì‹": 0.7, "ì•½ê´€": 0.7, "ê³„ì•½ì„œ": 0.7,
            "ë³´ì¦ì„œ": 0.7, "ì¦ëª…ì„œ": 0.7, "í™•ì¸ì„œ": 0.7, "ë™ì˜ì„œ": 0.7
        }
        
        # ì€í–‰ ì¼ë°˜ í‚¤ì›Œë“œ (ì¤‘ê°„ ê°€ì¤‘ì¹˜)
        bank_general_keywords = {
            "ëŒ€ì¶œ": 0.8, "ì˜ˆê¸ˆ": 0.8, "ì ê¸ˆ": 0.8, "í†µì¥": 0.7,
            "ê³„ì¢Œ": 0.7, "ì´ì": 0.7, "ê¸ˆë¦¬": 0.7, "ìˆ˜ìµë¥ ": 0.7,
            "ì†¡ê¸ˆ": 0.6, "ì´ì²´": 0.6, "í™˜ì „": 0.6, "ì™¸í™˜": 0.6,
            "ì¹´ë“œ": 0.6, "ë³´í—˜": 0.5, "íˆ¬ì": 0.5, "í€ë“œ": 0.5
        }
        
        # ì¡ë‹´ í‚¤ì›Œë“œ (ë‚®ì€ ê°€ì¤‘ì¹˜)
        chitchat_keywords = {
            "ë°¥": 0.3, "ë¨¹ì—ˆ": 0.3, "ë‚ ì”¨": 0.2, "ì˜í™”": 0.2,
            "ìŒì•…": 0.2, "ê²Œì„": 0.2, "ì—°ì• ": 0.1, "ì£¼ì‹": 0.1,
            "ì•ˆë…•": 0.4, "í•˜ì´": 0.4, "hello": 0.4
        }
        
        # ì ìˆ˜ ê³„ì‚°
        onboarding_score = sum(weight for keyword, weight in onboarding_keywords.items() 
                              if keyword in query_lower)
        bank_general_score = sum(weight for keyword, weight in bank_general_keywords.items() 
                                if keyword in query_lower)
        chitchat_score = sum(weight for keyword, weight in chitchat_keywords.items() 
                            if keyword in query_lower)
        
        # ì •ê·œí™” (0-1 ë²”ìœ„)
        total_score = onboarding_score + bank_general_score + chitchat_score
        if total_score > 0:
            onboarding_score /= total_score
            bank_general_score /= total_score
            chitchat_score /= total_score
        
        # ì˜ë„ ê²°ì • (ì„ê³„ê°’ ì™„í™”)
        if chitchat_score > 0.4:  # ì¡ë‹´ ì„ê³„ê°’ ë†’ì„
            intent = "chitchat"
        elif onboarding_score > 0.2 or bank_general_score > 0.2:  # ì˜¨ë³´ë”©/ì€í–‰ì¼ë°˜ ì„ê³„ê°’ ë‚®ì¶¤
            if onboarding_score >= bank_general_score:
                intent = "onboarding"
            else:
                intent = "bank_general"
        else:
            intent = "unknown"
        
        return {
            "intent": intent,
            "onboarding_score": onboarding_score,
            "bank_general_score": bank_general_score,
            "chitchat_score": chitchat_score
        }
    
    def _expand_query(self, query: str) -> List[str]:
        """
        ì¿¼ë¦¬ í™•ì¥ - ë™ì˜ì–´, í‚¤ì›Œë“œ ì¶”ê°€
        """
        query_lower = query.lower()
        expanded_queries = [query]
        
        # ì€í–‰ ì—…ë¬´ ë™ì˜ì–´ ë§¤í•‘
        synonyms = {
            "ëŒ€ì¶œ": ["ì—¬ì‹ ", "ì‹ ìš©", "ë³´ì¦", "ëŒ€í™˜", "ë³´ì¦ê¸ˆ"],
            "ì˜ˆê¸ˆ": ["ìˆ˜ì‹ ", "ì ê¸ˆ", "í†µì¥", "ê³„ì¢Œ", "ì…ê¸ˆ"],
            "ì†¡ê¸ˆ": ["ì´ì²´", "í™˜ì „", "ì™¸í™˜", "í•´ì™¸ì†¡ê¸ˆ"],
            "ì‹¬ì‚¬": ["ì‹¬ì˜", "ê²€í† ", "í‰ê°€", "ìŠ¹ì¸"],
            "ì„œë¥˜": ["ë¬¸ì„œ", "ì¦ë¹™", "ìë£Œ", "ì„œë¥˜"],
            "ë³´ë¥˜": ["ì¤‘ë‹¨", "ì§€ì—°", "ì—°ê¸°", "ë³´ë¥˜"],
            "í”¼í•´": ["ì†ì‹¤", "ì‚¬ê¸°", "í”¼í•´êµ¬ì œ", "êµ¬ì œ"],
            "ìˆ˜ìµë¥ ": ["ì´ì", "ê¸ˆë¦¬", "ìˆ˜ìµ", "ì´ìœ¨"],
            "cd": ["ì˜ˆê¸ˆì¦ì„œ", "ì •ê¸°ì˜ˆê¸ˆ", "cdìˆ˜ìµë¥ "]
        }
        
        # ë™ì˜ì–´ë¡œ ì¿¼ë¦¬ í™•ì¥
        for original, synonyms_list in synonyms.items():
            if original in query_lower:
                for synonym in synonyms_list:
                    expanded_query = query_lower.replace(original, synonym)
                    if expanded_query not in expanded_queries:
                        expanded_queries.append(expanded_query)
        
        return expanded_queries
    
    def _title_gated_search(self, question: str, intent: str, k: int = 20) -> List[Dict]:
        """
        íƒ€ì´í‹€ ê²Œì´íŒ…ì„ í†µí•œ ê²€ìƒ‰
        ì§§ì€ ì§ˆì˜ë‚˜ í¼ í‚¤ì›Œë“œê°€ í¬í•¨ëœ ê²½ìš° ì œëª© í›„ë³´ë¥¼ ë¨¼ì € í•„í„°ë§
        """
        # ì§§ì€ ì§ˆì˜ ë˜ëŠ” í¼ í‚¤ì›Œë“œ ê°ì§€
        short = len(question.strip()) <= 6
        form_keywords = ["ì•½ê´€", "ì–‘ì‹", "ì„œì‹", "ì‹ ì²­ì„œ", "ì„¤ëª…ì„œ", "ìœ„ì„ì¥", "í™•ì¸ì„œ", "ë™ì˜ì„œ"]
        has_form_keyword = any(kw in question for kw in form_keywords)
        
        title_shortlist = []
        if short or has_form_keyword:
            title_shortlist = title_candidates(question, LEXICON)
            print(f"íƒ€ì´í‹€ ê²Œì´íŒ… í›„ë³´: {title_shortlist}")
        
        # doc_type ìš°ì„ ìˆœìœ„
        doc_type_prior = DOC_TYPE_PRIORITIES.get(intent, [])
        print(f"doc_type ìš°ì„ ìˆœìœ„: {doc_type_prior}")
        
        # ê²€ìƒ‰ ì¿¼ë¦¬ êµ¬ì„±
        query = f"""
        SELECT d.id, d.title, d.category, d.description, d.file_path, d.upload_date,
               dc.content, dc.chunk_index, dc.metadata
        FROM documents d
        JOIN document_chunks dc ON d.id = dc.document_id
        WHERE d.is_indexed = true
        """
        
        # íƒ€ì´í‹€ í•„í„°ë§
        if title_shortlist:
            title_conditions = " OR ".join([f"d.title LIKE '%{title}%'" for title in title_shortlist])
            query += f" AND ({title_conditions})"
        
        # doc_type í•„í„°ë§
        if doc_type_prior:
            type_conditions = " OR ".join([f"d.category = '{cat}'" for cat in doc_type_prior])
            query += f" AND ({type_conditions})"
        
        query += f" ORDER BY d.upload_date DESC LIMIT {k * 3}"
        
        try:
            result = self.session.execute(text(query))
            rows = result.fetchall()
            
            documents = []
            for row in rows:
                doc = {
                    'id': row[0],
                    'title': row[1],
                    'category': row[2],
                    'description': row[3],
                    'file_path': row[4],
                    'upload_date': row[5],
                    'content': row[6],
                    'chunk_index': row[7],
                    'metadata': row[8] if row[8] else {},
                    'similarity': 0.8 if title_shortlist and any(title in row[1] for title in title_shortlist) else 0.5
                }
                documents.append(doc)
            
            print(f"íƒ€ì´í‹€ ê²Œì´íŒ… ê²€ìƒ‰ ê²°ê³¼: {len(documents)}ê°œ")
            return documents[:k]
            
        except Exception as e:
            print(f"íƒ€ì´í‹€ ê²Œì´íŒ… ê²€ìƒ‰ ì˜¤ë¥˜: {e}")
            return []
    
    def _get_dynamic_threshold(self, query: str) -> tuple[float, float]:
        """
        ë™ì  ì„ê³„ê°’ - ëŒ€í­ ì™„í™”í•˜ì—¬ GPT ë°±ì—… ì ê·¹ í™œìš©
        """
        if len(query.strip()) <= 4:  # 'ìœ„ì„ì¥', 'ì•½ê´€' ë“±
            return (0.05, 0.30)  # ë§¤ìš° ë‚®ì€ ì„ê³„ê°’ìœ¼ë¡œ í†µê³¼ìœ¨ ëŒ€í­â†‘
        return (0.10, 0.40)  # ì¼ë°˜ ì§ˆì˜ë„ ë‚®ì€ ì„ê³„ê°’
    
    def _is_greeting(self, query: str) -> bool:
        """
        ì¸ì‚¬ë§ ê°ì§€ - ì§§ì€ ì¸ì‚¬ë§Œ
        """
        greeting_words = {"ì•ˆë…•", "ì•ˆë…•í•˜ì„¸ìš”", "ã…ã…‡", "hi", "hello", "í•˜ì´"}
        return query.strip().lower() in greeting_words
    
    def _get_doc_type_template(self, doc_type: str) -> str:
        """
        ë¬¸ì„œ íƒ€ì…ë³„ ë‹µë³€ í…œí”Œë¦¿
        """
        templates = {
            "form": """
ë‹¤ìš´ë¡œë“œ ê²½ë¡œ: [íŒŒì¼ ê²½ë¡œ]
í•„ìˆ˜ í•­ëª©: [ì‘ì„±í•´ì•¼ í•  í•„ìˆ˜ ì‚¬í•­ë“¤]
ì‘ì„± ìš”ë ¹: [í˜•ì‹, ê¸¸ì´, ì£¼ì˜ì‚¬í•­]
ì œì¶œ ì±„ë„: [ì œì¶œ ë°©ë²• ë° ê²½ë¡œ]
ì œì¶œ ê¸°í•œ: [ê¸°í•œì´ ìˆë‹¤ë©´]
ë¬¸ì˜ì²˜: [ê´€ë ¨ ë¶€ì„œ ì—°ë½ì²˜]
""",
            "terms": """
í•µì‹¬ ìš”ì•½: [ì•½ê´€ì˜ ì£¼ìš” ë‚´ìš©]
ì˜ˆì™¸Â·ì£¼ì˜: [íŠ¹ë³„íˆ ì£¼ì˜í•  ì ë“¤]
ì •í™• ì¸ìš©: [ë¬¸ì„œëª…Â·ì„¹ì…˜Â·í˜ì´ì§€Â·ì¡°í•­]
ì‹œí–‰ì¼: [ì•½ê´€ ì‹œí–‰ì¼]
""",
            "product_brochure": """
ëŒ€ìƒ: [ìƒí’ˆ ì´ìš© ëŒ€ìƒ]
í•œë„: [ëŒ€ì¶œ/ì˜ˆê¸ˆ í•œë„]
ê¸ˆë¦¬: [ì ìš© ê¸ˆë¦¬]
ìˆ˜ìˆ˜ë£Œ: [ê´€ë ¨ ìˆ˜ìˆ˜ë£Œ]
í•„ìˆ˜ì„œë¥˜: [ì‹ ì²­ ì‹œ í•„ìš” ì„œë¥˜]
ì£¼ì˜ì‚¬í•­: [ì¤‘ìš”í•œ ì£¼ì˜ì ]
ë²„ì „/ì‹œí–‰ì¼: [ìƒí’ˆ ì„¤ëª…ì„œ ë²„ì „]
""",
            "regulation": """
ì¡°ë¬¸ ìš”ì•½: [ë²•ë ¹ì˜ ì£¼ìš” ë‚´ìš©]
í•´ì„ í¬ì¸íŠ¸: [ì¤‘ìš”í•œ í•´ì„ ì‚¬í•­]
ì‹œí–‰ì¼: [ë²•ë ¹ ì‹œí–‰ì¼]
ìƒìœ„-í•˜ìœ„ ê·œì •: [ê´€ë ¨ ë²•ë ¹ ê´€ê³„]
""",
            "manual": """
ì„œë¹„ìŠ¤ ê°œìš”: [ì„œë¹„ìŠ¤ ì£¼ìš” ë‚´ìš©]
ì´ìš© ë°©ë²•: [ì„œë¹„ìŠ¤ ì´ìš© ì ˆì°¨]
ì£¼ì˜ì‚¬í•­: [ì´ìš© ì‹œ ì£¼ì˜ì ]
ë¬¸ì˜ì²˜: [ì„œë¹„ìŠ¤ ê´€ë ¨ ë¬¸ì˜ì²˜]
"""
        }
        return templates.get(doc_type, "ì¼ë°˜ ì•ˆë‚´: [ë¬¸ì„œ ë‚´ìš© ìš”ì•½]")
    
    def _expand_forms_query(self, query: str) -> list:
        """
        ì–‘ì‹/ì„œì‹ ì „ìš© ì¿¼ë¦¬ í™•ì¥
        """
        query_lower = query.lower()
        expanded_queries = [query]
        
        # ì–‘ì‹/ì„œì‹ ë³„ì¹­ ì‚¬ì „
        form_aliases = {
            "ìœ„ì„ì¥": ["ìœ„ì„ì¥ ì–‘ì‹", "ìœ„ì„ì¥ ì„œì‹", "ìœ„ì„ì¥ ë‹¤ìš´ë¡œë“œ", "ìœ„ì„ì¥ ì œì¶œ", "ìœ„ì„ì¥ ì‘ì„±ë°©ë²•"],
            "ì´ì˜ì‹ ì²­ì„œ": ["ì´ì˜ì‹ ì²­ì„œ ì–‘ì‹", "ì´ì˜ì‹ ì²­ì„œ ì„œì‹", "ì´ì˜ì‹ ì²­ì„œ ì œì¶œ", "ì´ì˜ì‹ ì²­ ì‚¬ìœ ì„œ", "ì´ì˜ì‹ ì²­ ì ˆì°¨"],
            "ì‹ ì²­ì„œ": ["ì‹ ì²­ì„œ ì–‘ì‹", "ì‹ ì²­ì„œ ì„œì‹", "ì‹ ì²­ì„œ ë‹¤ìš´ë¡œë“œ", "ì‹ ì²­ì„œ ì œì¶œ", "ì‹ ì²­ì„œ ì‘ì„±ë°©ë²•"],
            "ë³´ì¦ì„œ": ["ë³´ì¦ì„œ ì–‘ì‹", "ë³´ì¦ì„œ ì„œì‹", "ë³´ì¦ì„œ ë‹¤ìš´ë¡œë“œ", "ë³´ì¦ì„œ ì œì¶œ"],
            "ì¦ëª…ì„œ": ["ì¦ëª…ì„œ ì–‘ì‹", "ì¦ëª…ì„œ ì„œì‹", "ì¦ëª…ì„œ ë°œê¸‰", "ì¦ëª…ì„œ ì‹ ì²­"],
            "í™•ì¸ì„œ": ["í™•ì¸ì„œ ì–‘ì‹", "í™•ì¸ì„œ ì„œì‹", "í™•ì¸ì„œ ë°œê¸‰", "í™•ì¸ì„œ ì‹ ì²­"],
            "ë™ì˜ì„œ": ["ë™ì˜ì„œ ì–‘ì‹", "ë™ì˜ì„œ ì„œì‹", "ë™ì˜ì„œ ì‘ì„±", "ë™ì˜ì„œ ì œì¶œ"]
        }
        
        # ë³„ì¹­ìœ¼ë¡œ ì¿¼ë¦¬ í™•ì¥
        for original, aliases in form_aliases.items():
            if original in query_lower:
                expanded_queries.extend(aliases)
        
        return expanded_queries
    
    def _classify_doc_type(self, title: str, content: str = "") -> str:
        """
        ë¬¸ì„œ íƒ€ì… ë¶„ë¥˜ - form, product_brochure, terms, regulation, faq, manual
        """
        title_lower = title.lower()
        content_lower = content.lower()
        
        # ì–‘ì‹/ì„œì‹ (form)
        if any(keyword in title_lower for keyword in ["ì‹ ì²­ì„œ", "í•´ì§€ì„œ", "ë³€ê²½ì„œ", "ë™ì˜ì„œ", "ìœ„ì„ì¥", "ë³´ì¦ì„œ", "ì¦ëª…ì„œ", "í™•ì¸ì„œ", "ì–‘ì‹", "ì„œì‹"]):
            return "form"
        
        # ìƒí’ˆì„¤ëª…ì„œ (product_brochure)
        if any(keyword in title_lower for keyword in ["ìƒí’ˆì„¤ëª…ì„œ", "ìƒí’ˆì•ˆë‚´", "ìƒí’ˆê°€ì´ë“œ", "ìƒí’ˆì†Œê°œ", "ìƒí’ˆ"]):
            return "product_brochure"
        
        # ì•½ê´€/ë‚´ê·œ (terms)
        if any(keyword in title_lower for keyword in ["ì•½ê´€", "ë‚´ê·œ", "ê·œì •", "ì •ì±…", "ê¸°ë³¸ì•½ê´€", "ê±°ë˜ì•½ê´€"]):
            return "terms"
        
        # ë²•ê·œ/ê°ë…ê·œì • (regulation)
        if any(keyword in title_lower for keyword in ["ë²•", "ì‹œí–‰ë ¹", "ê°ë…ê·œì •", "ë²•ê·œ", "ë²•ë ¹"]):
            return "regulation"
        
        # FAQ/ê³µì§€ (faq)
        if any(keyword in title_lower for keyword in ["faq", "ê³µì§€", "ì•ˆë‚´", "ë¬¸ì˜", "ì§ˆë¬¸", "ë‹µë³€"]):
            return "faq"
        
        # ì—…ë¬´ë§¤ë‰´ì–¼/ì‹œìŠ¤í…œ (manual)
        if any(keyword in title_lower for keyword in ["ë§¤ë‰´ì–¼", "ê°€ì´ë“œ", "ì‚¬ìš©ë²•", "ì‹œìŠ¤í…œ", "ì—…ë¬´", "ì ˆì°¨"]):
            return "manual"
        
        # ê¸°ë³¸ê°’
        return "general"
    
    def _get_doc_type_priority(self, intent: str) -> list:
        """
        ì˜ë„ë³„ ìš°ì„  ë¬¸ì„œ íƒ€ì… ë¦¬ìŠ¤íŠ¸
        """
        priority_map = {
            "onboarding": ["form", "terms", "manual", "faq", "regulation", "product_brochure"],
            "bank_general": ["product_brochure", "faq", "terms", "form", "manual", "regulation"],
            "unknown": ["faq", "product_brochure", "terms", "form", "manual", "regulation"]
        }
        return priority_map.get(intent, ["faq", "product_brochure", "terms", "form", "manual", "regulation"])
    
    def _get_answer_template(self, doc_type: str) -> str:
        """
        ë¬¸ì„œ íƒ€ì…ë³„ ë‹µë³€ í…œí”Œë¦¿
        """
        templates = {
            "form": """
â€¢ í•„ìˆ˜ í•„ë“œ: [í•„ìˆ˜ í•­ëª©ë“¤]
â€¢ ì‘ì„± ê·œì¹™: [í˜•ì‹/ê¸¸ì´ ì œí•œ]
â€¢ ì œì¶œ ê²½ë¡œ: [ì œì¶œ ë°©ë²•]
â€¢ ë‹¤ìš´ë¡œë“œ: [ë§í¬ ë˜ëŠ” ìœ„ì¹˜]
â€¢ ì£¼ì˜ì‚¬í•­: [íŠ¹ë³„í•œ ìš”êµ¬ì‚¬í•­]
""",
            "product_brochure": """
â€¢ ìƒí’ˆ ëŒ€ìƒ: [ëŒ€ìƒ ê³ ê°]
â€¢ í•œë„/ê¸ˆë¦¬: [ìˆ˜ì¹˜ ì •ë³´]
â€¢ ìˆ˜ìˆ˜ë£Œ: [ìˆ˜ìˆ˜ë£Œ êµ¬ì¡°]
â€¢ í•„ìˆ˜ì„œë¥˜: [ì œì¶œ ì„œë¥˜]
â€¢ ì˜ˆì™¸ì¡°ê±´: [ì£¼ì˜ì‚¬í•­]
â€¢ ìœ ì˜ì‚¬í•­: [ì¤‘ìš” ì•ˆë‚´]
""",
            "terms": """
â€¢ í•µì‹¬ ë‚´ìš©: [ì£¼ìš” ì¡°í•­ ìš”ì•½]
â€¢ ì˜ˆì™¸/ì£¼ì˜: [íŠ¹ë³„ ê·œì •]
â€¢ ìœ íš¨ê¸°ê°„: [ì‹œí–‰ì¼/ë§Œë£Œì¼]
â€¢ ê°œì •ì‚¬í•­: [ìµœê·¼ ë³€ê²½ë‚´ìš©]
â€¢ ê´€ë ¨ì¡°í•­: [ì—°ê´€ ê·œì •]
""",
            "regulation": """
â€¢ ë²•ì  ê·¼ê±°: [ê´€ë ¨ ë²•ë ¹]
â€¢ ì‹œí–‰ì¼: [íš¨ë ¥ ë°œìƒì¼]
â€¢ ì ìš©ë²”ìœ„: [ëŒ€ìƒ/ì˜ˆì™¸]
â€¢ ìœ„ë°˜ì‹œ ì¡°ì¹˜: [ì œì¬ ë‚´ìš©]
â€¢ ìµœì‹  ê°œì •: [ë³€ê²½ì‚¬í•­]
""",
            "faq": """
â€¢ ì§ˆë¬¸ ìš”ì•½: [í•µì‹¬ ë‚´ìš©]
â€¢ ë‹µë³€: [ìƒì„¸ ì„¤ëª…]
â€¢ ì ˆì°¨: [ë‹¨ê³„ë³„ ì•ˆë‚´]
â€¢ ê´€ë ¨ ë¬¸ì„œ: [ì°¸ê³  ìë£Œ]
â€¢ ë¬¸ì˜ì²˜: [ë‹´ë‹¹ ë¶€ì„œ/ì—°ë½ì²˜]
""",
            "manual": """
â€¢ ë‹¨ê³„ë³„ ì ˆì°¨: [1â†’2â†’3 ìˆœì„œ]
â€¢ í™”ë©´ ê²½ë¡œ: [ë©”ë‰´ ìœ„ì¹˜]
â€¢ ì£¼ì˜ì‚¬í•­: [ì˜¤ë¥˜ ë°©ì§€]
â€¢ ë¬¸ì œí•´ê²°: [ì˜¤ë¥˜ì½”ë“œ-í•´ê²°ì±…]
â€¢ ì¶”ê°€ ë„ì›€: [ì§€ì› ë°©ë²•]
"""
        }
        return templates.get(doc_type, """
â€¢ í•µì‹¬ ë‚´ìš©: [ì£¼ìš” ì •ë³´]
â€¢ ìƒì„¸ ì„¤ëª…: [êµ¬ì²´ì  ë‚´ìš©]
â€¢ ì£¼ì˜ì‚¬í•­: [ì¤‘ìš” ì•ˆë‚´]
â€¢ ê´€ë ¨ ì •ë³´: [ì¶”ê°€ ìë£Œ]
""")

    async def generate_rag_answer(self, question: str) -> Dict:
        """
        RAG ë‹µë³€ ìƒì„± ë©”ì„œë“œ
        """
        try:
            # ìœ ì‚¬ë„ ê²€ìƒ‰ìœ¼ë¡œ ê´€ë ¨ ë¬¸ì„œ ì°¾ê¸°
            try:
                similar_docs = await self.similarity_search(question, k=5)
                print(f"RAG ê²€ìƒ‰ ê²°ê³¼: {len(similar_docs)}ê°œ ë¬¸ì„œ ë°œê²¬")
                
                # ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìœ¼ë©´ í‚¤ì›Œë“œ ê¸°ë°˜ ê²€ìƒ‰ ì‹œë„
                if not similar_docs:
                    print("í‚¤ì›Œë“œ ê¸°ë°˜ ê²€ìƒ‰ ì‹œë„...")
                    keywords = self._extract_keywords(question)
                    for keyword in keywords:
                        similar_docs = await self.similarity_search(keyword, k=3)
                        if similar_docs:
                            print(f"í‚¤ì›Œë“œ '{keyword}'ë¡œ {len(similar_docs)}ê°œ ë¬¸ì„œ ë°œê²¬")
                            break
                            
            except Exception as e:
                print(f"RAG ê²€ìƒ‰ ì˜¤ë¥˜: {e}")
                similar_docs = []
            
            if not similar_docs:
                # ê´€ë ¨ ë¬¸ì„œê°€ ì—†ìœ¼ë©´ ì¼ë°˜ GPT ë‹µë³€
                print("ê´€ë ¨ ë¬¸ì„œ ì—†ìŒ - ì¼ë°˜ GPT ë‹µë³€ ìƒì„±")
                answer = self._call_gpt(f"ì§ˆë¬¸: {question}\n\nì€í–‰ ì—…ë¬´ì— ê´€ë ¨ëœ ë‹µë³€ì„ í•´ì£¼ì„¸ìš”.")
                return {
                    "answer": answer,
                    "sources": [],
                    "response_time": 0.0
                }
            
            # ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±
            context = "\n\n".join([
                f"[{doc['title']}]\n{doc['content']}"
                for doc in similar_docs
            ])
            
            # RAG í”„ë¡¬í”„íŠ¸
            prompt = f"""
ë‹¹ì‹ ì€ ì€í–‰ ì˜¨ë³´ë”© ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤. ğŸ»

ë‹¤ìŒ ê²€ìƒ‰ ì»¨í…ìŠ¤íŠ¸ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ë‹µë³€í•˜ì„¸ìš”:

{context}

ì§ˆë¬¸: {question}

ë‹µë³€ ìš”êµ¬ì‚¬í•­:
- ì‹ ì…ì‚¬ì›ì´ ì´í•´í•˜ê¸° ì‰½ê²Œ ì¹œì ˆí•˜ê³  ìƒì„¸í•˜ê²Œ ë‹µë³€í•˜ì„¸ìš”
- ìì—°ìŠ¤ëŸ¬ìš´ ë¬¸ë‹¨ìœ¼ë¡œ êµ¬ì„±í•˜ì—¬ ê¸¸ê²Œ ì„¤ëª…í•˜ì„¸ìš” (ë¶ˆë¦¿ í¬ì¸íŠ¸ë‚˜ ë‹¨ê³„ë³„ ë‚˜ì—´ ê¸ˆì§€)
- í•µì‹¬ ì •ë³´ë¥¼ ë¨¼ì € ì œì‹œí•˜ê³ , ì„¸ë¶€ì‚¬í•­ì„ í¬í•¨í•œ ì™„ì „í•œ ì„¤ëª…ì„ ì œê³µí•˜ì„¸ìš”
- ë¬¸ë‹¨ê³¼ ë¬¸ë‹¨ ì‚¬ì´ëŠ” ìì—°ìŠ¤ëŸ½ê²Œ ì—°ê²°í•˜ì—¬ íë¦„ ìˆê²Œ ì‘ì„±í•˜ì„¸ìš”
- í•œêµ­ì–´ë¡œ ì¹œê·¼í•˜ê²Œ ë‹µë³€í•˜ì„¸ìš” (ğŸ» ì´ëª¨í‹°ì½˜ì€ ë‹µë³€ ì‹œì‘ì—ë§Œ ì‚¬ìš©)
- AI í•˜ë¦¬ë³´ë¡œì„œ ì‹ ì…ì‚¬ì›ì„ ë„ì™€ì£¼ëŠ” ë§ˆìŒìœ¼ë¡œ ë‹µë³€í•˜ì„¸ìš”
- ë¬¸ì„œ ë‚´ìš©ì´ ìˆìœ¼ë©´ ë°˜ë“œì‹œ ë¬¸ì„œ ë‚´ìš©ì„ ìš°ì„  í™œìš©í•˜ê³ , ë¬¸ì„œ ë‚´ìš©ì— ì—†ëŠ” ë¶€ë¶„ë§Œ ì¼ë°˜ì ì¸ ì€í–‰ ì—…ë¬´ ì§€ì‹ìœ¼ë¡œ ë³´ì™„í•˜ì„¸ìš”
- ì§ˆë¬¸ê³¼ ê´€ë ¨ ì—†ëŠ” ë‚´ìš©ì€ ë‹µë³€ì— í¬í•¨í•˜ì§€ ë§ˆì„¸ìš”
- ì§ˆë¬¸ì˜ í•µì‹¬ í‚¤ì›Œë“œì™€ ì§ì ‘ ê´€ë ¨ëœ ë‚´ìš©ë§Œ ë‹µë³€í•˜ì„¸ìš”
- ë¬¸ì¥ì„ ì¶©ë¶„íˆ ê¸¸ê²Œ ì‘ì„±í•˜ì—¬ ìƒì„¸í•œ ì„¤ëª…ì„ ì œê³µí•˜ì„¸ìš”
- ê° ë¬¸ë‹¨ì€ 5-7ë¬¸ì¥ìœ¼ë¡œ êµ¬ì„±í•˜ì—¬ ì¶©ë¶„í•œ ì •ë³´ë¥¼ ì œê³µí•˜ì„¸ìš”
- ìƒí’ˆ ì¶”ì²œ ê´€ë ¨ ì§ˆë¬¸ì—ëŠ” êµ¬ì²´ì ì¸ ìƒí’ˆëª…ê³¼ íŠ¹ì§•ì„ í¬í•¨í•˜ì—¬ ë‹µë³€í•˜ì„¸ìš”
- ì—°ë ¹ëŒ€ë³„ ê³ ê° íŠ¹ì„±ì„ ê³ ë ¤í•œ ë§ì¶¤í˜• ìƒí’ˆ ì¶”ì²œì„ ì œê³µí•˜ì„¸ìš”
- ëŒ€ì¶œ ìƒí’ˆì˜ ê²½ìš° ê¸ˆë¦¬, í•œë„, ìƒí™˜ì¡°ê±´ ë“± êµ¬ì²´ì ì¸ ì •ë³´ë¥¼ í¬í•¨í•˜ì„¸ìš”
- ëŒ€ì¶œ ìƒë‹´ ì§ˆë¬¸ì—ëŠ” ì‹¤ì œ ëŒ€ì¶œ ìƒí’ˆì„ ì¶”ì²œí•˜ê³ , ì•Œë¦¼ ì„œë¹„ìŠ¤ë‚˜ ê¸°íƒ€ ì„œë¹„ìŠ¤ëŠ” ì¶”ì²œí•˜ì§€ ë§ˆì„¸ìš”
- ë‹µë³€ì„ ìì—°ìŠ¤ëŸ¬ìš´ ë¬¸ë‹¨ìœ¼ë¡œ êµ¬ì„±í•˜ì—¬ ê¸¸ê³  ìƒì„¸í•˜ê²Œ ì‘ì„±í•˜ì„¸ìš”
- ëŒ€ì¶œ ìƒí’ˆ ì¶”ì²œ ì‹œì—ëŠ” êµ¬ì²´ì ì¸ ìƒí’ˆëª…(ê°€ê³„ëŒ€ì¶œ, ì£¼íƒë‹´ë³´ëŒ€ì¶œ, ì „ì›”ì„¸ë³´ì¦ê¸ˆëŒ€ì¶œ ë“±)ì„ ëª…ì‹œí•˜ì„¸ìš”
- 70ëŒ€ ê³ ê°ì—ê²ŒëŠ” ì—°ë ¹ëŒ€ì— ë§ëŠ” ëŒ€ì¶œ ìƒí’ˆì„ êµ¬ì²´ì ìœ¼ë¡œ ì¶”ì²œí•˜ì„¸ìš”
- ëŒ€ì¶œ ìƒë‹´ ì§ˆë¬¸ì—ëŠ” ë°˜ë“œì‹œ êµ¬ì²´ì ì¸ ëŒ€ì¶œ ìƒí’ˆëª…ì„ ì¶”ì²œí•˜ê³ , ë‹¨ê³„ë³„ ì„¤ëª…ì´ë‚˜ ì¼ë°˜ì ì¸ ì¡°ì–¸ì€ í”¼í•˜ì„¸ìš”
- ê³ ê°ì—ê²Œ ì§ì ‘ì ì¸ ëŒ€ì¶œ ìƒí’ˆ ì¶”ì²œì„ ì œê³µí•˜ì„¸ìš”

ë‹µë³€:
"""
            
            answer = self._call_gpt(prompt)
            
            # ì°¸ê³ ìë£Œ êµ¬ì„±
            sources = [
                {
                    "title": doc["title"],
                    "content": doc["content"][:200] + "..." if len(doc["content"]) > 200 else doc["content"]
                }
                for doc in similar_docs
            ]
            
            # ì°¸ê³ ìë£Œë¥¼ ë‹µë³€ì— ì¶”ê°€
            if sources:
                answer += "\n\nì°¸ê³  ìë£Œ:\n"
                for source in sources:
                    answer += f"\nâ€¢ {source['title']}"
            
            return {
                "answer": answer,
                "sources": sources,
                "response_time": 0.0
            }
            
        except Exception as e:
            print(f"Generate RAG answer error: {e}")
            return {
                "answer": "ì•—, ì ê¹ë§Œìš”! ğŸ»\nì¼ì‹œì ì¸ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆì–´ìš”.\nì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.",
                "sources": [],
                "response_time": 0.0
            }

    async def process_query(self, question: str) -> Dict:
        """
        ì¿¼ë¦¬ ì²˜ë¦¬ ë©”ì¸ ë©”ì„œë“œ
        """
        try:
            # RAG ë‹µë³€ ìƒì„±
            result = await self.generate_rag_answer(question)
            
            return {
                "answer": result["answer"],
                "sources": result.get("sources", []),
                "response_time": result.get("response_time", 0.0)
            }
            
        except Exception as e:
            print(f"Process query error: {e}")
            return {
                "answer": "ì•—, ì ê¹ë§Œìš”! ğŸ»\nì¼ì‹œì ì¸ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆì–´ìš”.\nì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.",
                "sources": [],
                "response_time": 0.0
            }